This document describes the research plan for the project. It includes a
discussion of relevant context, the research questions and sketches of the
methods for answering them. As the proposal progresses, the document will
be populated with information about (i) the status of each research
question (answered/failed to answer); (ii) possible new questions and
method adjustments; and (iii) discussion of problems that we encountered
and how we solved them (or sidestep them).

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; CONTEXT  ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

Sound gradual typing faces two significant challenges:

 - the development cost of adding type annotations;
 - the computational cost of runtime checking these annotations.

Both stand in its way of developer adoption. 

The combined effect of these two challenges manifests itself during the
development of a mixed-typed code base. When a developer gets confronted with
such a code base, it could be for at least two different reasons:

 - someone started migrating pieces of code from the untyped to the typed world
 - someone added a typed piece of code to the untyped code repository.

How this mixed-typed code came about doesn't matter for the second challenge,
however. The injection of type-untype boundaries causes the addition of runtime
checks and thus impose a cost.  More precisely, after a potentially significant
investment of time into adding type annotations, a developer may realize that
the type annotations cause significant slow-downs to the code. Indeed, the
slow-downs might be so bad that the program is no longer unusable.

Assuming the developer is *not* willing to remove the types (and lose the
maintenance benefits they provide), the question becomes:

 - how to make the program fast (enough) again.

Research confirms that a fully-typed program can run _faster_ than the
originally untyped program (in some variants of gradual typing). But, equipping
an entire code base with type annotations is likely to represent a huge effort,
so going that far is too much work. Hence, the developer must contemplate

 - which component to type-annotate next to reduce the number of dynamic checks
   (and/or enable type-sensitive code optimizations).

Unfortunately, empirical evidence indicate that in many scenarios there is no
reliable strategy to find this next step. 

In response to this conundrum, PL researchers have opted to change the rules of
engagement. If the natural semantics is not viable, then some other semantics
may offer a solution. Two such proposals have emerged. The transient/shallow
semantics inlines strategic checks in typed code to avoid costly wrappers. The
concrete/nominal semantics limits the space of type annotations (and with them
the allowed interactions between typed and untyped code) to replace wrappers
with tags, which can be checked efficiently.

However, neither proposal has resulted in a
satisfying solution. The nominal/concrete approach leads to lower
performance slowdowns but hampers expressiveness significantly. The
transient approach improves performance compared to natural but only for
configurations with a small number of annotations. Specifically, while in
natural developers should expect to face a steep rise of checking cost as
they add annotations that drops steeply after a threshold (a bell shape),
in transient the slope of cost is smooth but cost is roughly
proportional to the number of type annotations and reaches its highest
point when all possible annotations have been added.  

Based on this latter observation, recently, Greenman [PLDI 2022] has
suggested  a mixed semantics: developers should be able to combine deep
and shallow checks (and no checks) to get the maximum type checking
benefits with the minimum performance penalty. Greenman's proposal has
been implemented for Typed Racket and initial empirical data is promising.
In detail, developers can choose if the types of a Typed Racket module
should be enforced deeply or shallowly when it interacts with untyped
code. Hence, for a program with N modules developers can choose between
3^N configurations of the modules of the program --- each module can be
untyped, deeply typed or shallowly typed. Preliminary experiments with the
GTP benchmarks show that the three-dimensional space allows developers to
bypass the ``dead'' areas of the two-dimenantional spaces of deep and shallow
semantics when considered separately. 

However, the three-dimensional world poses an important research problem.
The three-dimensional space is more complex that the two dimensional ones,
and bound to impose a higher cognitive load for developers when they
select which part of the code to type next. In other words, there is an
open research question about the pragmatics of the three-dimensional
world: how can developers navigate it?



;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;; RESEARCH QUESTION ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;


Q: How can developers navigate the three-dimensional space of type
migration?

Q': Can profiler data help developers navigate the
three-dimensional space of type migration?

Q'': What kind of profiler data can help developers
navigate the three-dimensional space of type migration?

[In the write up, we need here context about profiling techniques (in
Racket) --- bring across strengths and weaknesses]

Q''': Can a combination of feature and statistical profiler
data help developers navigate the three-dimensional space of
type migration?

Q'''': Can developers interpret data from feature and statistical
profilers to navigate the three-dimensional space of type migration?


Q''''': How do different strategies of interpreting feature and
statistical profiler data compare in terms of how they help developers
navigate the three-dimensional space with a given amount
of effort and a given performance threshold they can tolerate for the
result of the migration?

Q'''''': How do different strategies of interpreting feature and
statistical profiler data compare in terms of how they help developers
obtain a configuration of a given program where (i) a specific set of modules
are typed; (ii) at most K components overall are typed; and (iii) the
configuration has a slow down below a given threshold?



Important definitions
---------------------

++ Navigation : A path of configurations in the lattice of a program where
each configuration has more typed modules than the previous one.
[bg: why not 'migration path'?]

++ Migration step: One part of a navigation; a pair of configurations.

++ Migration strategy: A partial order on configurations. Each config
has a unique next. (You could also say that each config has some
possible nexts --- the ones in the profile --- and some impossible
nexts and one best next.)

++ Migration target: A set of components of a program that should be typed.
A migration target is fulfilled if these components have types.
[TODO: this is not quite right because current strategies have two targets, min
types and max overhead]

++ Successful navigation: A navigation that reaches a
configuration where (i) the migration target is fulfilled; 
(ii) at most K components overall are typed; and (iii) the
configuration has a slow down below a given threshold?

For our experiments, each path is chosen by a strategy --- which is to say
that each step is locally optimal for the strategy. This means that every
start configuration and strategy leads to a unique path.

In reality, a developer might not follow a set path. He can take a step,
revert it, and try another one. (We don't model this because we want to
compare the effectiveness of certain strategies.)


;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; METHOD ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

The plan is to follow a rational program approach: 

 1) Construction of scenarios. We will use the GTP benchmarks.  The
 potential placements of type annotations in the benchmarks induces  a
 three-dimensional lattice for each benchmark. Each configuration of the
 lattice that has higher slowdown than the threshold and where the
 migration target is fulfilled but no other components are typed. 

 Note: Starting from configurations where the migration target is
 fulfilled and all other components are untyped is sufficient. If the
 slowdown of this configuration is below the slowdown threshold then the
 scenario is uninteresting. If the slowdown is above the threshold,
 given that the target must be fulfilled eventually, the goal is to
 determine what other components besides the typed one need to become
 typed or untyped in order for the slowdown to fall below the threshold.
 After all, the slowdown of a configuration does not depend on how the
 configuration was constructed, it is an intrinsic characteristic of the
 configuration. 

 2) Construction of modes: 
      +++ We will use domain knowledge and the existing types of the modules
      to encode the possible ways a developer can interact with a program
      after a profiling round.
      +++ We will construct one mode per strategy. Each mode's strategy
      corresponds to a function that consumes profiling data and the
      current configuration, and computes how the configuration should
      change (or that there is no next configuration). Each mode uses its
      strategy function to construct the next configuration, or to declare
      success or failure. See dedicated Strategies section for details and
      examples of strategies.
      +++ Failure condition for all modes: A mode produces no
      configuration that is below the accepted performance threshold
      within the tolerated number of typing steps.
      +++ Success condition for all modes: A mode produces a configuration
      that is below the accepted performance threshold within the
      tolerated number of typing steps.

 3) Mode comparison: 
      ++ Success or Failure comparison
      ++ Navigation length (maybe weighted by type annotation burden) for
      scenarios where all modes succeed.

 4) Data collection:
      (i) We construct the lattice for each benchmark and measure the
      performance of each configuration for all the lattices without
      producing profiler data. This acts as ground truth for the
      performance of the different configurations.
      - [quadT] is running on Cloudlab Utah using m510 nodes and the default
        ubuntu OS. Each node has an Intel Xeon D-1548 CPU running at 2.0 GHz
        and 64 GB RAM.
      - [morsecode forth fsm fsmoo mbta sieve acquire jpeg kcfa lnm snake
        suffixtree take5 tetris zombie synth] all ran on Cloudlab Wisconsin
        using c220g1 nodes and the default ubuntu OS. Each node has two
        Intel E5-2630 CPUs running at 2.40 GHz and 128GB RAM.
      - [gregor quadU zordoz] have no data: gregor and quadU are large with
        good overall performance; zordoz cannot run some shallow/deep mixes.
        Hope to fix zordoz soon.

      (ii) We construct the set of scenarios. See the strategies section.

      (ii) Starting from the scenarios, we use the profiler data 
      to construct the navigations.

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; STRATEGIES ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

The rational programmer can use the following info:

1) the feedback of the profiler(s),
2) the characteristics of the current config, and
3) the characteristics of the untyped config
   (e.g., to find the overhead of the current config relative to untyped)

Hence, the space of strategies can be described as the set of functions
from the profiler feedback and the current scenario to an action.
Roughly, an action corresponds to the rational programmer's next step
given the available information; it can be either a description of what
component must change and how, or a declaration of failure if no change is
possible under the given strategy.  Specifically, an action is either
FAILURE, or it consists of two pieces: a component name and the (new)
type-checking semantics for the component (U,S or D).


At a high level, each strategy function aims to improve how the current
configuration fares against a specific metric of configuration
``goodness''. Such metrics are (imperfect) proxies of how close is the
current configuration to the  actual goal of reaching a configuration that
has performance below the target threshold. In contrast though to the
actual goal, these metrics can be expressed in terms of the available
information/inputs to the strategy function and, thus, they can be the
basis for the construction of feasible (but sub-optimal) actions. 

Given that the boundary and the statistical profiler
produce different feedback, the precise definition of the strategy function 
changes depending on the profiler used. However, analogous
strategy functions form groups of comparable strategies based on their metrics.  


The following is a list of the working groups of strategies:

1. Optimistic boundary elimination

Metric: A configuration C has a higher score than a
configuration D if the most expensive boundary from
D is eliminated in C, where an eliminated boundary is of the 
form DD. 

- [bnd] find the slowest boundary:
   + UD => DD
   + US => DD
   + SD => DD
   + SS => DD
- [prf] find the slowest module A that requires or provides to a B module with 
stricter checks, then pick the slowest B (total% or self%):
   + A = U and B = D => A = D
   + A = U and B = S => A = D and  B = D
   + A = S and B = D => A = D
   + A = S and B = S => A = D and  B = D

Intuition: Optimostic boundary-elimination tries to improve the most costly boundary.
With boundary-profile information, it is obvious which boundary
is most expensive. With statistical-profile info, we don't know
which boundary is the worst; and so, we guess by improving the interaction 
with a stricter neighbor B of the identified component A. The strategy
aims to turn the boundary into  DD to remove the cost of interaction
between the two sides of the boundary entirely. The boundary change can
though affect the interactions between A and other components.

Note: An alternative for the statistical-profile version is to turn into D all
the dependencies of A, but penalize that with a cost factor. 

2. Conservative boundary elimination  

Metric: A configuration C has a higher score than a
configuration D if the most expensive boundary from
D is eliminated in C, where an eliminated boundary is of the 
form SS. 


- [bnd] find the slowest boundary:
   + UD => SS
   + US => SS
   + SD => SS
- [prf] find the slowest module A that requires or provides to a B module with 
stricter checks, then pick the slowest B (total% or self%):
   + A = U and B = D => A = S and B = S
   + A = U and B = S => A = S 
   + A = S and B = D => B = S
   + A = D and B = D => A = S and B = S


Intuition: Conservative boundary-elimination tries to improve the most costly boundary.
With boundary-profile information, it is obvious which boundary
is most expensive. With statistical-profile info, we don't know
which boundary is the worst; and so, we guess by improving the interaction 
with a stricter neighbor B of the identified component A.  The strategy
aims to turn the boundary into SS to remove the cost of boundary crossings
between the two sides of the boundary. The cost of interactions between A
and B is not removed entirely due to the inlined checks in both
components. Similarly, these checks may inflict costs in the interaction
between A, B and other components, but the rationale behind the strategy
is that such interactions are less costly than the wrappers possibly
created by converting the boundary into a DD. 


Note: An alternative for statistical-profile version is to turn into S all
the dependencies of A, but penalize that with a cost factor. 



3a,3b. Cost-aware [optimistic/conservative] boundary elimination
Metric: Same as for boundary elimination but the metric takes into account
the cost of adding types to a component. Hence, the strategy prioritizes
boundaries where both sides are already typed. 


- [bnd] split boundaries into two groups: SD and SS boundaries, and the
  rest, then rank within the group based on profiling data, pick the
  top ranked boundary in the first group, or the top ranked boundary in
  the second group if the first group is empty,transform it according to
  the corresponding [prf] boundary elimination strategy (optimistic or
  conservative).

- [prf] split components into typed with at least one typed dependency or
  typed client, and the rest, rank within the two groups using profiler
  information,  pick the top ranked component in the first group, or the top
  ranked component in the second group if the first group is empty, transform
  it according to the corresponding [prf] boundary elimination strategy
  (optimistic or conservative).

Note 1: The same alternative for the statistical profiler corresponding boundary
elimination strategies is possible here.

Note 2: It is possible to generalize the metric to take into account the
variable cost of adding types. A proxy/metric for that could be the number
of types need to be added into a module and the overall ranking could be
determined based on a lexicographic sorting of boundaries/components
according to (Cost-from-type-additions, profiler information).

4a,4b. Configuration-aware [optimistic/conservative] boundary elimination
Metric: Same as for boundary elimination but the metric takes into account
the how many components in the current configuration are already typed.
If this number is above a threshold then the chances of performance
degradation due to a ``domino'' effect from D's wrappers is smaller than
if the number is below the threshold. 

- [bnd] behave same as [bnd] conservative boundary elimination if configuration
  above threshold, and same as [bnd] optimistic if below.  

- [prf] behave same as [prf] conservative boundary elimination if configuration
  above threshold, and same as [prf] optimistic if below.  


Note: The strategy can also be combined with cost-aware to form a
configuration-aware, cost-aware boundary elimination.


5a,b. Profiler-agnostic random boundary elimination
Metric: A configuration A has a higher score if one of its components has
stricter type checking semantics. 


- [bnd] pick a boundary and adjust it [ optimistically/
  conservatively] 

- [prf] pick a component and adjust it [optimistically/
   conservatively]  


Intuition: These strategies act as baselines for the value of the profiler
information for the corresponding boundary elimination strategies.


Note: The selection of the boundary or component can also take into
account cost.

6. Profiler-agnostic all boundary flipping
Metric: A configuration A has a higher score than configuration B if all
components of B at S[D] are D[S] in  A. 

- Flip the strictness for already typed components in a configuration. 

Intuition: This strategy acts as a baseline for a comparison of profiler-based
strategies with a very low effort ``blind'' adjustment.


;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; MORE IDEAS ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;


;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;;;;;;;;;;;;;;;;;;;;;;;; THREATS TO VALIDITY ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
 

 1) Selection of benchmarks.
 2) Selection of migration targets for the construction of scenarios.
 3) Unaccounted overhead due to profiling.
 4) Set of considered strategies is not exhaustive (for instance,
 strategies that use metrics that use the whole history of the navigation
 so far, or strategies that use a combination of metrics, or strategies
 that back track).
