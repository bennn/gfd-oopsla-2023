OOPSLA 2023 Paper #201 Reviews and Comments
===========================================================================
Paper #201 How Profilers Can Help Navigate Type Migration


Review #201A
===========================================================================

Overall merit
-------------
C. Weak reject

Reviewer expertise
------------------
X. Expert

Paper summary
-------------
This paper is concerned with plausible migration paths for migratory typing. As individual configurations of mixed type annotations in gradually-typed programs can experience unacceptably high performance overhead, it is an important question whether there are reasonable paths to migrate a program from a less typed to a more typed configuration via intermediate steps with acceptable overheads, and if so, how to find such paths, as at many points, a programmer has many choices of where to focus there efforts for adding type annotations. The authors propose using profilers, and different kinds of them, to indicate which part of a program should be annotated next. They investigate this idea for the gradual typing performance benchmark suite assembled for (Typed) Racket, which increases the search-space by allowing to toggle type-checks between "shallow" and "deep" modes, but makes it (mostly) tractable by only toggling types and their modes on whole modules. The end result in the particular setup chosen by the authors is that only the boundary-check profiler seems to be able to provide helpful guidance in a significant number of cases, if at all, and using "shallow" types almost never leads to a performance improvement.

Assessment
----------
### Pros
+ Important Problem
+ Accessible

### Cons
- Limited Generality
- Inadequate exploration of even the limited scope
- The talk about the "rational programmer" method obfuscates a rather simple methodology

I want to begin by stressing that I agree with the authors that finding smooth migration paths in gradually-typed programs is an important problem, and tools and techniques that guide programmers through this process would be useful. However, as I understand the work presented in this paper, it does not provide a significant improvement over the state of the art or provide enough significant insights.

The paper claims two contributions in particular, which I cite and address in reverse order:
> At the meta level, the application of the rational programmer method to the performance problems of type migration provides one more piece of evidence about its usefulness.

This may be a mismatch between different subcommunities, but as someone who is interested in the design and implementation of gradually-typed languages, the rational programmer method seemed to be a complicated way of talking about the most straightforward way of doing your experiments, which come down to: "In the migration lattice where each configuration is annotated with its performance relative to the fully untyped configuration, for each configuration where the performance is worse than some threshold, does the output of profiler X indicate a (path to a) reachable "next" configuration whose performance is either better than the threshold or at least better than right now?". Talking about a rational programmer and satisficing behavior and such seems to obfuscate more than illuminating anything.

Maybe the fact that this experimental setup matches the scheme of the rational programmer provides evidence for its usefulness, but it seems to me that that should be much more of a side-note in either a paper about the method itself or in the related work section here, but the paper seems to present this as much more novel than it is.

> At the object level, the results of the rational programmer experiment provide guidance to developers about how to best use the feedback from profilers during type migration.

While technically true, this quite a weak statement. Said guidance comes down to "always add deep types to the less-typed side of the highest-overhead boundary that is found by the boundary profiler", because that leads to success in about half of the cases explored here.
The bigger meaning implicit in the paper is that using a profiler in this way is actually a useful strategy, which is not really borne out because of the fact that a large chunk of configurations - almost half are hopeless to fix in the first place, a fact that is well-known, as is the fact that for many of the programs in the GTP benchmark suite in Typed Racket, there simply is no reasonable path through the migration lattice that avoids unacceptable overheads. As such, the experiment was in some sense doomed from the start. There are several smaller issues with the overall experimental setup and design choices that I will expand upon more in the comments for authors, but the last key bit is that as presented, the experiments are tailored to Racket and do not obviously generalize to other gradually-typed languages, and even there, they do not work that well.

Comments for authors
--------------------
I do appreciate the focus on paths, since their existence and an ability to find them seems much more important in practice than the overall distribution of acceptable-performance configurations that the original gradual typing performance evaluation work focuses on. However, the choice of the GTP benchmark suite is a bit odd, since there it has already been established that reasonable paths often do not exist. Maybe one other way of looking at my criticism here is to ask: "Who is helped by this?" - your introduction makes it partially sound like this is a useful tool for actual programmers. In that case, an actual user-study might have been a better fit; as it would have gotten you more realistic benchmarks to work with, and, well, actual users. In that sense, I actually disagree with the usefulness of the "rational programmer" method here. On the other hand, in my interpretation, the GTP benchmark suite is addressed at language designers and implementers, demonstrating particular pitfalls and areas for improvement in gradual typing systems. As someone from that corner, my main takeaway from this paper are a few more hard numbers on the unsurprising fact that there are many low-performing configurations that are hard to get out of. To really be helpful here, a little more exploration might have helped, to see how far we really are away from any sort of findable reasonable path (also, given that you know the full lattice, you could have also compared against the "optimal" strategy), and what other criteria than the more or less randomly chosen 3x overhead for success and 1x overhead hopefulness there could be (this is particularly noticeable in the `take5` benchmark, which Greenman reports as getting to a worst shallow overhead of 2.99x, and a worst combined overhead of 2.97x; similarly, `dungeon` has a worst-case overhead of 15000x in with deep types, 4.97x with only shallow types, and 3.16x with a combination of the two; again close to the boundary; finally, `mbta` has worst-case overheads below 2x all the time, though your graphs are still empty because they are all deemed hopeless).

As you report, running all the benchmarks fully takes quite a lot of time, but it seems to me that the bulk of that work was just reproducing the numbers from earlier papers on your own hardware. I admit that the raw numbers do not seem to be easily available online, but did you contact the authors of the earlier evaluation papers to ask them if they could share those with you? That would have saved you months of computation time that you could have spent on just running the profilers on interesting configurations (those that constituted performance-debugging-scenarios in the original data), and thus enabled you to also do this for the larger ones (gregor, quadT, quadU). I know that there's the possibility that your measurements might be significantly different from those of earlier publications, but in order for them to be valid experiments without running each benchmark on multiple different machines, we already have to believe that that is not going to happen.

Even if exploring the larger benchmarks was computationally infeasible, they might still have yielded an interesting test case for some random exploration: measure the fully untyped configuration. Then pick some random subset of the configurations, including the fully untyped one. Now, for each of them, pick a random migration until you get into a performance debugging scenario, then apply your method for addressing it. How far do you get?

Questions for author response
-----------------------------
- You claim that a lesson for language designers is that "the addition of shallow type enforcement does not seem to help with the navigation of the migration lattice" (lines 842-844), but as far as I can tell, some benchmarks that do seem to show significant improvements in Greenman's data (`take5`, `mbta`, `dungeon`) have been effectively excluded from your experiment because of your definition of "hopelessness". How does that square with your above claim?
- Related to the above, how does your definition of hopelessness actually work? As far as I understand it, you look at the lattice to see if there is even any future configuration (within some distance?) that has less than 1x overhead, which excludes programs where there always is overhead for some reason. How does this apply to `take5`, `mbta`, and `dungen`, which seem to have reasonable worst-case overheads?
- You acknowledge the threat to validity that you may not have covered all possible strategies, but claim that in response to this threat you designed your strategies "carefully". I can easily imagine combinations of strategies working better and having some other heuristic to decide between them, though admittedly your choice of benchmarks would make it hard to be successful for any of them. Why did you not compare to an "optimal" strategy that just looks at reachability in the lattice, and what exactly did being "careful" in your design entail?
- Is there something about the "rational programmer" experiment that is not obvious and that I missed?
- How do you imagine your results being used?



Review #201B
===========================================================================

Overall merit
-------------
A. Accept

Reviewer expertise
------------------
X. Expert

Paper summary
-------------
This paper presents a "reasonable programmer" study comparing two
different profilers being used to guide conversion of Racket modules
from untyped to typed. After a general introduction, section 2 talks
about profiling, and section 3 about the "rational programmer"
methodology. Section 4 presents the experimental design, section 5 the
results of a massive benchmarking effort, which are discussed in
section 6.  Section 7 covers related work, and section 8 concludes:
"Onward! Onward! ever Onward! IBM!"

Assessment
----------
I found the paper generally easy to follow and well written - I only
had to read it once! The problem is pretty well set out, the
methodology and experimental design clearly explained, and then the
results are likewise presented clearly. Given that questions about how
best to introduce gradual / migratory / option / pluggable types into
programming languages remains an important research question; along
with related questions of how best to implement those types, and what
other tooling is needed, I think this paper makes a solid scientific
contribution, so I hope to see it presented at OOPSLA.  My comments
below should be taken more as "suggestions for improvement" to an
already good paper - not as things that are actually wrong with this
draft.

Comments for authors
--------------------


not sure why "functional langauges" is a keyword

line 24 and many places elsewhere - there's an assumption in the
writing that e.g. shallow or transient checks will always slow down
a program linearly. Blowing my students' trumperts, Roberts et al 2016
shows that's shouldn't be the case for  jitted implementation.


reflecting on that point - there are also complementary implementation
assumptions coming through in the "lessons' section. what I mean here
is he assumption that typed code is always going to be faster than
untyped code. That's obviously the case in Racket (either version, I
guess you're running on top of Chez, probably worth mentioning) but
not necessarily the case. So again, unfortunately things are more
implementation dependent than we would like.


a few short!  very short!  code examples showing deep vs shallow type
checks might help.  Also how modules set their type checking policy?


111 be good to know module granularity earlier in the intro

and then towards the end, I'd be really interested to know any
thoughts or even opinions about granularity questions.  If a utility
module has a hundred strihg functions, but an application uses only
one ("leftpad", say) doesn't that make it more expensive to fix if
that module isn't typed - because you'd have to type every function in
the module?  alternatively could you refactor the module or even put a
typed version of leftpad ihto a separate module, type that one, then
bind the rest of the code to use just that one?

dunno why I thought about MLKit's extensive use or profiling for
memory, but I did.  The issue there - I think - is that good MLKit
performance depends on the region inference which is *implicit* in the
source code - unless you print out the AST post region inference,
which has *explicit* regions, or look at the profiler to find out
what's going wrong, which again identifies *explicit* regions.

howmuch of the difficulty here is that especially "deep" typechekcs
are implicit in Racket source.  if they were made explcit (dunno how)
would that help? 

could the boundary profiler just count the number of crossings, rather
than the time?  


216 often faster, so sometime it isn't :-)

Fig2 not clear the terms "conservative" and "optimistic" help here.
isn't "optimistic" just deep and "conservative" shallow?

333 rational programmers  -- but rational programmers aren't
programmers, they're *not* people!  could reword to avoid that
implication.  Yes, I'm being really picky.  yes that's because there's
an implcit bias to longer reviews.


386 or elsewhwere - is it worth talking about the difficulty -
especailly of the philosphy implied by the "migratory" approach, that
programmers can just annotation types into their programs *without
restructuring that code* - i.e. without following an implicit model of
typed programming - writing code that can in principle be typed but
has not been.  Again this is implementation dependent.  One example
because I'm old (and yes, I know Racket has option types which would
do this job) --- 30 years ago, in Self, say you wantedw to record an
*optional integer*.  Classic dyanmic langauge would be ot put a null
in the slot, or false, or something. Self's advice was to have a
Boolean slot, and then a separate integer slot, separating them by
type, befause otherwise the VM's type predictors would get
confused. That is: write dynamically typed code as if it were typed.
while Racket's type system is indeed a wonderous thing, there must
surely be cases where the available types are inadequate to describe a
program's behaviour.

550 cite / expand GPT benchmarks &^ justify the choice

584 WOW!  that's a serious benchmarking effort!! 

howdo you know you got "reliable performance measures" on the cloud?
did you test for that?
how do you know you're out of VM warmup effects?
(see Edd Barrett et al, OOPSLA 2017).

If I take 561-564 at face value, it seems you did 11 runs; it seems
there's an assumption neither boundary nor statistical profiler would
interact with the VM or optimiser; simimlarly your measurements are
just one run each for stat and boundary profiler for each
configuraiton?   Is that right?  if not, fix it!
if so, can you justify it?   why not do 1 startup run,
4 genreal runs, three runs with each profiler? 

I thought Fig9 was a misprint.  Cute, but perhaps always put figures
after the first mention in the tet


To encourage accountability, I'm signing my reviews in 2023.
For the record, I am James Noble, kjx@acm.org

Questions for author response
-----------------------------

* I don't seem to have many questions.
I made this one up:

* does boundary profiling really line up with deep types,
and shallow profiling really line up with shallow types?
Not sure why I got that impression early on, but I did.

* OK there is the "leftpad" question below.

* and this one: how can programmers tell if their slowdown is caused by
  type-checking vs by a slow algorithm. Perhaps I wrote an O(N^2)
  leftpad?  e.g. at line 388 - reseachers care, but why would
  programmers care whether the slowness is caused by their code or the
  typechecking?   Would e.g. an op[tion to *turn off all checking* at
  the module level - e.g. treat migratory types as if they were
  optionala* help answer this question?
  is that answer implementation dependent?

* like that one, other questions are really just suggestions so I
I'm putting the rest in the suggestions list.

* how accurate is this cynical take: typed racket generates the best
  code when fully ("deeply") typed and gradual typing imposes costs at
  module boundaries. Therefore: to speed up code, use the boundary
  profiler, and make things deeply typed.



Review #201C
===========================================================================

Overall merit
-------------
B. Weak accept

Reviewer expertise
------------------
Z. Outsider

Paper summary
-------------
Paper 201 applies the rational agent abstraction, familiar from economics, to the problem of type migration in gradually-typed languages.  Given a partially-typed program that is performing poorly due to the overhead of runtime type checks, a programmer (who does not wish to undo any typing efforts already performed) can in many cases improve the program's performance by adding additional type constraints. Unfortunately, the space of choices is large: any untyped component is a candidate, there is more than one way to add type checks at runtime, and the checks interact in difficult-to-predict ways.  The programmer has access to oracles that reveal whether a choice improved performance – profilers – but there is more than one kind of profiler too.  How should a programmer navigate the 3**n latice of partially-typed programs?

The paper sets up this model, defines the search space formally, defines a way to meaningfully compare two strategies, and ultimately reports on a collection of experiments that apply strategies to traverse the search space, given a set of established benchmarks. Useful rules-of-thumb are extracted.

Assessment
----------
Thank you for submitting this paper to OOPSLA.  I learned a lot reading it.  I would like to see this paper published because it is a good piece of thinking and writing (modulo some minor issues I raise below regarding claims and the formalism). 

At the end of the day, however, I am not highly confident in my rating because, as an outsider to this area, I do not understand how important/impactful the results are.  See Q1.

Comments for authors
--------------------
In several places, the draft makes universal assertions about fully-typed programs that use all deep types.  In Section 2, starting with a running example of a partially-typed program (hence one whose performance is presumably impacted by type checking overhead at runtime), the draft asserts "adding deep types to every module is the option that is almost always guaranteed to solve the problem."  This assumption is important, because it means you can navigate the program lattice monotonically. Later, in Section 3, the draft says "the fully-typed program with deep types is often faster than its untyped counterpart."  This is a slightly different statement – it asserts that more types is the remedy not only for the performance overhead of partial typing but also a move towards better performance compared to any untyped program.  I am afraid that I am just a systems programmer and not a user of gradually-typed Racket, so I struggle to understand why these assertions are certainly true.  It seems to me that adding runtime checks adds overhead, and I need help understanding why it sometimes doesn't.

Section 4.2 – "the 3**n configurations of [the lattice] are ordered: Pi < Pj iff Pj has at least one component that is untyped in Pj."  Putting aside what appears to be a typo, this still does not appear to say what I think the authors intend.  I think you probably mean something like Pi < Pj iff for all components c, if c is typed in Pi it is typed in Pj, AND there exists some component c' that is typed in Pj but not in Pi.  Otherwise, I am confused.

Although this is not the sort of evaluation that I am accustomed to reading, I found it very interesting.

The threats to validity and and prior work sections are simply excellent.

Questions for author response
-----------------------------
Q1: How do these results generalize beyond the walled garden of gradually-typed Racket?  

Q2: More specifically, is "boundary profiler" really an 'approach' in the same sense that "statistical profilers" are, or is it a specific artifact?
