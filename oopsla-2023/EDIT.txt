The main changes to the paper are:

- Abstract and section 1 clarify that the main result of the experiment
  points to deep types for costly boundaries as the most successful strategy.
  They also give a concrete success rate for this strategy. Finally, section 1
  explains why this result is a surprise given prior preliminary work (see
  contributions list).
- Section 1 expands the list of contributions to include the collection of
  data about migration paths.
- Section 1 points to the benefits of JITs for shallow/transient. Section
  7 references work that supports these benefits, and section 8 mentions
  that future work may improve the results of this paper.
- Section 2 expands on the background on gradual typing and checking strategies.
- Section 4.3 explains why prior work does not answer the research
  questions of this paper.
- Section 5 gives more information about data collection.
- Section 8 lists and clarifies general takeaways.

Besides the above, we have improved the prose in minor ways throughout.

What follows are pointers from the remarks in the reviews to places in the
paper that respond to those remarks. Line numbers refer to the Revision
PDF, not to the diff PDF.


> Review #201A
> ===========================================================================
> * Updated: 23 Jun 2023 8:33:44pm EDT
> 
> Update after the rebuttal
> -------------------------
> ### Response to four "incorrect statements"
> I'll grant that I don't quite know anymore why I wrote that it is well-known
> that a large chunk of configurations are hopeless to fix. For whether any
> useful paths exist, there are the two papers (jfp'19 and pldi'22) you mentioned
> that give actual numbers. Now you state that that is a totally different
> question from what you were asking, and I am not really convinced that it is.
> Just looking at  Figure 3 in the original popl'16 paper, which shows the
> lattice for the `suffixtree` benchmark (6 modules), shows that there is no good
> path through the lattice from the bottom, and that the overheads are not just
> contained in a single layer (i.e. any bad configuration closer to the untyped
> configuration will still encounter more bad configurations on the way to the
> top). As such, while well-known fact might be hyperbole, it's certainly a
> widespread expectation.

The POPL '16 paper is about 2^N lattices, not 3^N.  The paragraph starting at
line 558 compares directly to prior work.

> Finally, on the reproducing earlier numbers, you make some good points; in
> particular, on closely re-reading Greenman's paper, it seems true that he
> actually did not measure all 3^N configurations. The newer versions of Racket
> argument is fair, though if the older numbers existed might still be trumped. 

line 80 compares our data to that of prior work (first contribution).

> ### About the rational programmer method
> I think we may be talking past each other here. 
> 
> > This argument skims over the key question that the rational programmer lets
> > us answer: how to choose a "next" configuration. 
> 
> I thought that 
> 
> > does the output of profiler X indicate a (path to a) reachable "next"
> > configuration whose performance is either better than the threshold or at least
> > better than right now?
> 
> captures that.

As sections 2 and 3 explain, the interpretation of profiler output is not
straightforward. The rational programmer method, where the design of
comparable strategies (including baselines) plays an integral part,
provides guidance on how to impose structure on the problem.

> > On the contrary, it was the rational programmer method that provided the
> > template and tools for designing the experiment. Prior work contemplated
> > migration questions (popl'16, jfp'19) but lacked a compelling way to answer
> > them. This work managed to do it by instantiating the rational programmer.
> 
> "I am at location X in the lattice and want to add more types without making
> the program too slow. I don't just want to try out all possible ways of adding
> types to different parts to decide which is best, so something that indicates
> where my efforts might best be focused would help me decide." still seems like
> the obvious way to do it. The results of the paper show that that strategy's
> success rate is still not great, and I claim that that in the given setting
> that was obvious from the start, which may have been the reason why no
> published work has considered it so far. So my point is that this paper lacks a
> compelling way to answer migration questions, too.

line 89 and line 564 explain why the most successful strategy was not obvious
from the start.

Also as the abstract and section 5 describe: (1) the strategy is
effective in 50% of scenarios when accepting 2 setbacks; (2) the number of
setbacks is a good indicator for when a developer should stop relying on
profiling for migration; and (3) excluding hopeless configurations, the success
rate goes up to 70%. So yes, the results are not ideal, but the strategy is a
pragmatic approach that fares better than previous proposals.

> ### Other comments
> > The results were a significant surprise to us -- and we have a large amount
> > of experience with the design, implementation, and evaluation of gradually
> > typed languages. Based on Greenman's work (pldi'22) and Siek's extensive work
> > on Reticulated Python, we fully expected shallow types to help in the lower
> > half of the lattice. We will state this starting hypothesis in the paper.
> 
> Okay, based on the information that you are the first to actually measure the
> 3^N lattice, that would actually be a contribution of the paper, and should be
> listed as such, even though it is kind of incidental to the rest of the work.

line 80 lists the contribution as requested. 

> > The overwhelming success of TypeScript and underwhelming success of sound
> > gradual typing suggests that, at the moment, only gradual typing researchers
> > and Typed Racket enthusiasts (which includes commercial users) put up with
> > slowdowns.
> 
> I agree that it's unlikely that the slowdowns inherent in gradually-typed
> Racket are acceptable to many users, but this argument makes it sound like
> original JavaScript and original Racket had the same levels of popularity and
> people then picked one of their typed versions based on their implementation of
> gradual typing...
> 
> What I was trying to say is that from an experimental design point, choosing
> these hard cutoffs was unfortunate. The original graphs in the popl'16 paper
> tried to avoid such cutoffs, instead trying to plot how far one would get with
> whatever cutoff one would want to make (and then the text settled on
> 3-deliverable as an example benchmark, which people have then used to justify
> the 3x cutoff). For benchmarks like `take5`, `dungeon`, and `mbta`, it would
> still have been interesting to see if you could get to a better place, even if
> that place was not as good as one might hope.

Adding a cutoff dimension on top of the looseness dimension would
complicate the presentation of the data immensely. Moreover, line 694
explains that considering cutoffs around 3x does not affect the conclusions
of the experiment.

The artifact (under review) will allow other researchers to examine our data
using any cutoff they want.

> > User studies do not answer the same questions as the rational programmer.
> > They could tell us what strategies some individual (and small number of) users
> > choose to interpret profiler output, or whether a user can follow a specified
> > strategy. They are not suited to test the overall effectiveness of a strategy
> > because of the limited patience of humans and the huge number of confounding
> > variables involved. By contrast, the rational programmer can test the
> > effectiveness of a strategy and thereby recommend it (or not) to users -- on
> > tens of thousands of scenarios.
> 
> This again is about who this paper is for. As a tool, the profiler is only
> effective if actual users use it effectively. I agree that this does not answer
> the same question as the rational programmer. I was just saying that your
> framing made the paper sound as if it was about something useful for actual
> users.

lines 67 explains the relationship between user studies and rational
programmer experiments. The paper does not claim that the latter replace
the former; what it does state is that developers could execute similar
algorithms as the rational programmer and, if they did, the experiment
predicts their success rates. 

The explanation of the object-level contribution (line 86) clarifies this
point. Specifically for developers, it is worth mentioning that even if they do
not currently employ any strategic debugging algorithms, this does not mean
that they cannot learn from the results. Researchers and university educators
can and must explain its benefits for motivation and its mechanics for
instruction. At least, as educators, we should hope that this is the case.

> Review #201B
> ===========================================================================
> * Updated: 23 Jun 2023 9:35:37am EDT
> 
> Meta Review
> -----------
> After an in-depth discussion of the paper, the reviewers recognised the
> following arguments for the paper: 
> 
>  * **Pro** this paper experimentally confirms what we already expect: that
>    Racket adds overheads at "boundaries" between typed and untyped modules, so
>    it's best to profile module boundaries to see that overhead — although
>    there's less going on than the writing implies.
> 
>  * **Con** this paper experimentally confirms what we already expect: that
>    Racket adds overheads at "boundaries" between typed and untyped modules, so
>    it's best to profile module boundaries to see that overhead — although
>    there's less going on than the writing implies.
>
> While the experiment confirms the general expectation, actually having done the
> experiment still has value.  We recommend that the paper be conditionally
> accepted, subject ensuring all the changes are made from response, plus other
> similar issues are addressed in the final version.
> 
> We also recommend that the revised draft takes care to characterise the
> contributions of the paper accurately. The paper should make the simplicity of
> the basic idea clearer in the introduction, abstract, and conclusion (that
> Racket adds overheads at "boundaries" between typed and untyped modules, so
> it's best to profile module boundaries to see that overhead); to highlight the
> most valuable technical contribution: the large-scale investigation of
> migration paths, plus the full N^3 measurements.  The  claims listed in the
> introduction of the current draft are actually quite weak, while the claims
> implied by the way this draft is written are rather too strong.

See above for specific reactions. Additionally sections 1-4 of the revised
version have several prose simplifications. Section 2 adds an example.

Having said this, we must respond to the comment concerning boundaries in
Racket. Although it is correct that deep types impose module-boundary
costs, this is not the case for shallow types. The latter diffuse checks
(and their costs) throughout typed code in a way that is connected to any
specific boundary.

The paragraph starting at line 127, supplemented by an example starting at
line 209, explains how this diffusion of checks complicates the problem. At
the start of the experiment, we had expected that at least one of the
profilers would discover "hot" checks in shallow-type code, but as the
results show, this is not the case.

> Comments for authors
> --------------------
> 
> not sure why "functional langauges" is a keyword
> 
> line 24 and many places elsewhere - there's an assumption in the
> writing that e.g. shallow or transient checks will always slow down
> a program linearly. Blowing my students' trumperts, Roberts et al 2016
> shows that's shouldn't be the case for  jitted implementation.

line 31 discusses the assumption and adds a reference to Roberts.

> reflecting on that point - there are also complementary implementation
> assumptions coming through in the "lessons' section. what I mean here
> is he assumption that typed code is always going to be faster than
> untyped code. That's obviously the case in Racket (either version, I
> guess you're running on top of Chez, probably worth mentioning) but
> not necessarily the case. So again, unfortunately things are more
> implementation dependent than we would like.

line 627 states that we used Racket-on-Chez.

> a few short!  very short!  code examples showing deep vs shallow type
> checks might help.  Also how modules set their type checking policy?

line 209 adds a short example to illustrate the differences between
checking strategies.

line 181 briefly mentions that modules pick their type checking
policy by choosing a language (aka #lang)

> 111 be good to know module granularity earlier in the intro

line 103 explains the module granularity aspect of the paper's setting.

> and then towards the end, I'd be really interested to know any
> thoughts or even opinions about granularity questions.  If a utility
> module has a hundred strihg functions, but an application uses only
> one ("leftpad", say) doesn't that make it more expensive to fix if
> that module isn't typed - because you'd have to type every function in
> the module?  alternatively could you refactor the module or even put a
> typed version of leftpad ihto a separate module, type that one, then
> bind the rest of the code to use just that one?
>
> dunno why I thought about MLKit's extensive use or profiling for
> memory, but I did.  The issue there - I think - is that good MLKit
> performance depends on the region inference which is *implicit* in the
> source code - unless you print out the AST post region inference,
> which has *explicit* regions, or look at the profiler to find out
> what's going wrong, which again identifies *explicit* regions.
> 
> howmuch of the difficulty here is that especially "deep" typechekcs
> are implicit in Racket source.  if they were made explcit (dunno how)
> would that help? 
> 
> could the boundary profiler just count the number of crossings, rather
> than the time?  
> 
> 
> 216 often faster, so sometime it isn't :-)
> 
> Fig2 not clear the terms "conservative" and "optimistic" help here.
> isn't "optimistic" just deep and "conservative" shallow?

We consider ``optimistic'' and ``conservative'' terms that bring out the risks
that come with each strategy in a way that ``deep'' and ``shallow'' does
not (due to the strong association with the corresponding typing regimes).

Also, we are afraid that overloading 'deep' and 'shallow' (in _unrelated_
ways) would be hard on readers.

The paragraph starting at line 376 discusses this dimension of the
nomenclature. 


> 333 rational programmers  -- but rational programmers aren't
> programmers, they're *not* people!  could reword to avoid that
> implication.  Yes, I'm being really picky.  yes that's because there's
> an implcit bias to longer reviews.

The revised paper uses only ``developers'' to refer to humans. A footnote
on line 97 explains this to readers.

> 386 or elsewhwere - is it worth talking about the difficulty -
> especailly of the philosphy implied by the "migratory" approach, that
> programmers can just annotation types into their programs *without
> restructuring that code* - i.e. without following an implicit model of
> typed programming - writing code that can in principle be typed but
> has not been.  Again this is implementation dependent.  One example
> because I'm old (and yes, I know Racket has option types which would
> do this job) --- 30 years ago, in Self, say you wantedw to record an
> *optional integer*.  Classic dyanmic langauge would be ot put a null
> in the slot, or false, or something. Self's advice was to have a
> Boolean slot, and then a separate integer slot, separating them by
> type, befause otherwise the VM's type predictors would get
> confused. That is: write dynamically typed code as if it were typed.
> while Racket's type system is indeed a wonderous thing, there must
> surely be cases where the available types are inadequate to describe a
> program's behaviour.
> 
> 550 cite / expand GPT benchmarks &^ justify the choice

line 461 adds a reference to the GTP benchmarks (a newly published paper)
and justifies the choice.

> 584 WOW!  that's a serious benchmarking effort!! 
> 
> howdo you know you got "reliable performance measures" on the cloud?
> did you test for that?
> how do you know you're out of VM warmup effects?
> (see Edd Barrett et al, OOPSLA 2017).

line 623 explains that we made sure to reserve whole servers on CloudLab.

> If I take 561-564 at face value, it seems you did 11 runs; it seems
> there's an assumption neither boundary nor statistical profiler would
> interact with the VM or optimiser; simimlarly your measurements are
> just one run each for stat and boundary profiler for each
> configuraiton?   Is that right?  if not, fix it!
> if so, can you justify it?   why not do 1 startup run,
> 4 genreal runs, three runs with each profiler? 

The paragraph starting at line 920 adds this threat.

> I thought Fig9 was a misprint.  Cute, but perhaps always put figures
> after the first mention in the tet

Moved the figure to line 834, after the relevant prose

> To encourage accountability, I'm signing my reviews in 2023.
> For the record, I am James Noble, kjx@acm.org
> 
> Questions for author response
> -----------------------------
> * I don't seem to have many questions.
> I made this one up:
> 
> * does boundary profiling really line up with deep types,
> and shallow profiling really line up with shallow types?
> Not sure why I got that impression early on, but I did.

line 209 discusses profiling strategies and type checking regimes.

> * OK there is the "leftpad" question below.
> 
> * and this one: how can programmers tell if their slowdown is caused by
>   type-checking vs by a slow algorithm. Perhaps I wrote an O(N^2)
>   leftpad?  e.g. at line 388 - reseachers care, but why would
>   programmers care whether the slowness is caused by their code or the
>   typechecking?   Would e.g. an op[tion to *turn off all checking* at
>   the module level - e.g. treat migratory types as if they were
>   optionala* help answer this question?
>   is that answer implementation dependent?
> 
> * like that one, other questions are really just suggestions so I
> I'm putting the rest in the suggestions list.
> 
> * how accurate is this cynical take: typed racket generates the best
>   code when fully ("deeply") typed and gradual typing imposes costs at
>   module boundaries. Therefore: to speed up code, use the boundary
>   profiler, and make things deeply typed.
> 
> 
> 
> Review #201C
> ===========================================================================
> Comments for authors
> --------------------
> In several places, the draft makes universal assertions about fully-typed
> programs that use all deep types.  In Section 2, starting with a running
> example of a partially-typed program (hence one whose performance is presumably
> impacted by type checking overhead at runtime), the draft asserts "adding deep
> types to every module is the option that is almost always guaranteed to solve
> the problem."  This assumption is important, because it means you can navigate
> the program lattice monotonically. Later, in Section 3, the draft says "the
> fully-typed program with deep types is often faster than its untyped
> counterpart."  This is a slightly different statement – it asserts that more
> types is the remedy not only for the performance overhead of partial typing but
> also a move towards better performance compared to any untyped program.  I am
> afraid that I am just a systems programmer and not a user of gradually-typed
> Racket, so I struggle to understand why these assertions are certainly true.
> It seems to me that adding runtime checks adds overhead, and I need help
> understanding why it sometimes doesn't.

The long paragraph starting at line 116 expands on deep and shallow
checks. In addition the revision of the prose of section 2 eliminates
dependencies on much background knowledge.

> Section 4.2 – "the 3**n configurations of [the lattice] are ordered: Pi < Pj
> iff Pj has at least one component that is untyped in Pj."  Putting aside what
> appears to be a typo, this still does not appear to say what I think the
> authors intend.  I think you probably mean something like Pi < Pj iff for all
> components c, if c is typed in Pi it is typed in Pj, AND there exists some
> component c' that is typed in Pj but not in Pi.  Otherwise, I am confused.

line 480 fixes the issue. Thanks!

