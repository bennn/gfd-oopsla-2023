\newcommand{\numgtp}{16}
\newcommand{\gtpurl}{\url{https://docs.racket-lang.org/gtp-benchmarks/index.html}}
\newcommand{\boundaryMB}{536} % 536244 kb
\newcommand{\statisticalMB}{4645} % 4645788 kb
\newcommand{\runtimeMB}{44}

This section presents the results of a rational programmer
experiment that compares 17 strategies on programs from
the GTP benchmark suite~\cite{gtp-benchmarks}.


\subsection{Experiment}

\subsubsection{Benchmarks}

The GTP benchmarks are a collection of small to mid-sized Typed Racket programs
that support a full lattice of mixed-typed interactions.
Since the benchmarks originate from useful scripts
and exercise a variety of types at module boundaries~\cite{gtnffvf-jfp-2019},\footnote{\gtpurl{}}
they are an appropriate testbed for comparing navigation strategies.
Configurations with high overhead due to deep types should benefit from
feature-specific profiling, while configurations that suffer from shallow
types may benefit more from the statistical profiler.

The experiment uses the v7.0 release of the GTP Benchmarks with small modifications
to support feature-specific profiling.
Version 7.0 is outdated, but was the latest version when we began collecting
data in Summer 2022~(\cref{sec:data}).
The modifications add static information so that the profiler can peek through
\emph{adaptor modules} in specific benchmarks.
They do not change the run-time behavior of the code.
In total, nine benchmarks are affected~(\cref{s:adaptor-rewrite}).
Source code for the original benchmarks and our modified ones will be available
in the artifact for this paper.

We omit four of the 21 benchmarks:
\bmname{zordoz} because it currently cannot run all deep/shallow/untyped configurations
due to a known issue;\footnote{\url{https://github.com/bennn/gtp-benchmarks/issues/46}}
\bmname{gregor},\bmname{quadT}, and \bmname{quadU} because each has over 1.5
million configurations and we lack the resources to run a full experiment;\footnote{
\textbf{To OOPSLA reviewers}: we have collected performance data for
\bmname{quadT} (which has the worst performance of the three) and hope to
collect profile results in the coming weeks.  We have no plans to measure
\bmname{gregor} or \bmname{quadU}.
}
and \bmname{sieve} because it is very small, with only four configurations.


\subsubsection{Profiling Tools}

Racket ships with tools for statistical~[CITE] and feature-specific
profiling~\cite{}.
The feature-specific profiler is aware of contracts as a default feature.
Since deep gradual types compile to contracts,
Since deep gradual types compile to contracts, this default contract
mode is suitable for the experiment.

The experiment uses slightly-modified variants of these default tools
to make their output easier to analyze.
First, the modified statistical profile renders its
findings as a JSON graph rather than as a table~(see~\cref{f:fsm-code:statistical}).
Second, the modified feature-specific profiler groups contracts
by boundary (using the two sides of their blame objects) and sorts groups
in descending order.
Thus, the most expensive \emph{boundary} appears at the top rather than the single
most expensive contract~(shown in~\cref{f:fsm-code:boundary}).


\subsubsection{Data}
\label{sec:data}

The ground-truth data for our experiment consists of running times,
feature-specific profile output, and statistical profile output.
Collecting this data required three basic steps for each configuration
of the \numgtp{} benchmarks:
\begin{enumerate}
  \item
    Run the configuration once, ignoring the result, to warm up the JIT.
    Then run eight more times measuring performance.
    In particular, we recorded {cpu time}s as reported by the Racket
    \code{time} function.
  \item
    Install the modified feature-specific profiler and run it once,
    collecting output in a file.
  \item
    Install the modified statistical profiler and run it once, collecting output.
\end{enumerate}
Complications arose because of the large scale of the experiment, and because
we discovered and fixed two issues in Typed Racket along the way.


In total, we analyzed 116,154 configurations
and recorded 1,277,694 measurements.
%% $11 * \totalnumconfigs{} = \totalnummeasurements{}$
All collection took place on CloudLab~\cite{cloudlab} servers using a recent
%% https://github.com/racket/typed-racket/pull/1316
version of Racket (as of July 2022).
We utilized one core per server in an effort to obtain reliable performance
measurements.
The dataset takes up 5GB overall, most of which is due to the statistical
profiler (4.6GB).
Measurements began in July 2022 and have continued to the present (April 2023)
because we are working on the 4.7 million configurations of \bmname{quadT}.

Initially, we planned to collect running times for all configurations including
those of \bmname{quadT} before moving on to the profilers.
Due to this ambitious plan and the popularity of resources on CloudLab,
we spread our experiment across three clusters.
\Cref{t:data-collection} (top) matches datasets to clusters.
Most running times came from \machinename{c220g1} machines managed by the University of Wisconsin-Madison.
Most profiler results (feature-specific and statistical) came from \machinename{m510} machines
managed by the University of Utah.
The specifications of these machines are reported at the bottom of~\cref{t:data-collection}.

Both the \bmname{dungeon} and \bmname{morsecode} benchmarks took a different
path and used only one machine type: \machinename{c220g2} and
\machinename{m510}, respectively.
This is because we collected their data much later than the other benchmarks,
after discovering and fixing two issues in Typed Racket that significantly
affected their performance.\footnote{\url{https://github.com/racket/typed-racket/pull/1282}, \url{https://github.com/racket/typed-racket/pull/1316}}
For \bmname{dungeon}, we used a newer Typed Racket commit.
For \bmname{morsecode}, we cherry-picked one small commit onto the default Typed Racket
for Racket v8.6.0.2 (to be available in our artifact).
Fixing these issues \emph{was not necessary} for the rational programmer experiment.
The only reason for re-running was to stay current with major changes to Typed Racket.

\begin{table}[t]
  \caption{Datasets, their origin, and server details}
  \label{t:data-collection}

  \begin{tabular}{llll}
    Dataset           & Server & Racket & Typed Racket \\\midrule
    dungeon           & \machinename{c220g2} & \stdrkt{} &  \commitname{29ea3c10}{29ea3c105e0bd60b88c1fd195b54fa716863f690} \\
    morsecode         & \machinename{m510}   & same & \commitname{700506ca}{700506ca01393f211229101c47d8420f6d535de9} (cherry pick) \\
 %% quadT runtime  & \machinename{m510}   & same & same \\
    other runtime     & \machinename{c220g1} & same & default \\
    other profile      & \machinename{m510}   & same & same
  \end{tabular}

  \bigskip

  \begin{tabular}{llrrr}
    %% multi-cpu, multi-core machines .. but we didn't use that, right?
    Server & Site & CPU Speed & RAM & Disk \\\midrule
    \machinename{c220g1} & Wisconsin & 2.4GHz & 128GB & 480GB SSD \\
    \machinename{c220g2} & Wisconsin & 2.6GHz & 160GB & 480GB SSD \\
    \machinename{m510}   & Utah      & 2.0GHz &  64GB & 256GB SSD
  \end{tabular}
\end{table}


\subsection{Interesting Scenarios}

For our experiment, we consider every configuration as a potential
performance debugging scenario.
The goal for each scenario is to find another configuration
with at least as many typed modules that runs no slower than the
untyped configuration.

Naturally, some configurations are uninteresting as starting
points because they already run fast enough.
The untyped configuration is one, by definition.
There should ideally be many interesting points in a benchmark
and a handful of uninteresting ones to navigate to.
\Cref{t:baseline-trouble} tabulates this landscape for the benchmarks.
All but two have plenty of interesting scenarios: over 80\% of
all configurations have some overhead relative to untyped.
Navigating to a successful configuration is likely to be
challenging for the rational programmer.

The other two benchmarks, \bmname{fsm} and \bmname{lnm}, have
fewer intesting scenarios.
In \bmname{lnm}, only 40\% of the configurations
are interesting.
This suggests that the rational programmer will have an easier
time navigating to a successful configuration in \bmname{lnm}
because there are many to choose from.


\begin{table}[t]
  \caption{How many of the $3^N$ configurations have any overhead to begin with?}
  \label{t:baseline-trouble}
  \begin{tabular}[t]{l@{\qquad}l}
    \begin{tabular}[t]{lrr}
      Benchmark           & $3^N$ & \% Interesting \\\midrule
      \bmname{morsecode}  &    81 & 82.72\% \\
      \bmname{forth}      &    81 & 93.83\% \\
      \ycell{\bmname{fsm}}        &    \ycell{81} & \ycell{76.54\%} \\
      \bmname{fsmoo}      &    81 & 83.95\% \\
      \bmname{mbta}       &    81 & 88.89\% \\
      \bmname{zombie}     &    81 & 91.36\% \\
      \bmname{dungeon}    &   243 & 99.59\% \\
      \bmname{jpeg}       &   243 & 94.65\% \\
    \end{tabular}
    &
    \begin{tabular}[t]{lrr}
      Benchmark           & $3^N$ & \% Interesting \\\midrule
      \ycell{\bmname{lnm}}        &   \ycell{729} & \ycell{40.47\%} \\
      \bmname{suffixtree} &   729 & 98.49\% \\
      \bmname{kcfa}       &  2,187 & 92.87\% \\
      \bmname{snake}      &  6,561 & 99.97\% \\
      \bmname{take5}      &  6,561 & 99.95\% \\
      \bmname{acquire}    & 19,683 & 99.23\% \\
      \bmname{tetris}     & 19,683 & 95.47\% \\
      \bmname{synth}      & 59,049 & 99.99\%
    \end{tabular}
  \end{tabular}
\end{table}


\subsection{Running a Rational Programmer}

We have instantiated the 17 strategies from~\cref{subsec:strategies} as
programs that navigate toward fast configurations.
Fifteen strategies use profile information as decribed in~\cref{f:bstrategies} and~\cref{f:cstrategies}.
The remaining two are profile agnostic: random boundary and toggling.

\Cref{f:strategy-overall} presents the results of running all strategies on all
interesting scenarios.
This data lets us answer the first (parameterized) research question:

\begin{itemize}
  \item[$Q_X$] How successful is a strategy $X$ with the elimination of
    performance bottlenecks?
\end{itemize}

Each stacked bar in the ``skyline'' of \cref{f:strategy-overall} provides
several answers.
The lowest, widest part of each bar counts how many configurations
achieved a strict success.
The next three stories count $1$-loose, $2$-loose, and $3$-loose successes.
The striped spire counts $N$-loose successes.
And the antenna uses an even weaker notion, strict 3x success, which we return
to in a moment.

Turning now to the various strategies:
\begin{itemize}
  \item
    \emph{Optimistic} navigation performs well when guided by the \emph{feature-specific} profiler,
    finding strict success in almost 40\% of all configurations.
    With a $2$-loose relaxation, success rises to above 50\%.
    The results are far worse, however, with \emph{statistical (total)} or \emph{statistical (self)}
    profiles, both of which perform comparably.

  \item
    \emph{Cost-aware optimistic} is almost as successful as optimistic when driven
    by \emph{feature-specific} profiles and equally successful with \emph{statistical (total)}
    and \emph{statistical (self)}.

  \item
    \emph{Conservative} navigation is unsuccessful, no matter what profiler it uses.

  \item
    \emph{Cost-aware conservative} is unsuccessful as well.
    Even with $N$-loose relaxation, only a handful of configurations achieve success.

  \item
    \emph{Configuration-aware optimistic} navigation with
    \emph{feature-specific} profiles succeeds in approximately 36\% of all
    configurations under strict and just ovef 50\% with $3$-loose.
    With \emph{statistical (total)} and \emph{statistical (self)} profiles,
    the success rate drops to 10\% even for $N$-loose.

  \item
    \emph{Random boundary} succeeds in 5\% of all configuration---far more than the conservative strategies.
    Allowing for 1,2,3-loose success improves the rate incrementally, and an $N$-loose search
    succeeds in nearly 40\% of configurations.

  \item
    \emph{Toggling} achieves strict success more often than random, in roughly 6\% of all configurations.
    ($N$-loose is meaningless for toggle because it steps to at most one configuration.)
\end{itemize}

All told, \emph{optimistic, feature-specific} navigation is the most likely to
succeed on an arbitrary configuration.
In the context of a single benchmark the results may vary~(\cref{s:bm-sky}),
but not for most of them.

Now, back to the antennae.
An antenna rises above a bar when there exists scenarios that can reach a 3x overhead
relative to untyped without any degredations along the path.
The number 3x is the classic Takikawa constant for acceptable gradual typing
overhead~\cite{tfgnvf-popl-2016,vss-popl-2017,bbst-oopsla-2017}.
We use it to allow some wiggle room for shallow types, which rarely achieve a
1x overhead in Typed Racket~\cite{g-deep-shallow}.

FILL


\begin{figure}[t]
  \includegraphics[width=\columnwidth]{data/strategy-overall-feasible.pdf}
  \caption{How many of the 114,428 scenarios does each strategy succeed in, for six notions of success.}
  \label{f:strategy-overall}
\end{figure}


\subsection{Black Holes}

\begin{table}[t]
  \caption{How scenarios can possibly reach 1x without removing types?}
  \label{t:blackhole}
  \begin{tabular}[t]{l@{\qquad}l}
    \begin{tabular}[t]{lrr}
      Benchmark                &  \# Scenario &  \% Hopeful \\\midrule
      \bmname{morsecode}       &           67 &    100\% \\
      \bmname{forth}           &           76 &     36.84\% \\
      \bmname{fsm}             &           62 &    100\% \\
      \bmname{fsmoo}           &           68 &    100\% \\
      \rcell{\bmname{mbta}}    &   \rcell{72} & \rcell{0\%} \\
      \bmname{zombie}          &           74 &     35.14\% \\
      \rcell{\bmname{dungeon}} &  \rcell{242} & \rcell{0\%} \\
      \bmname{jpeg}            &          230 &    100\%
    \end{tabular}
    &
    \begin{tabular}[t]{lrr}
      Benchmark                &   \# Scenario &  \% Hopeful \\\midrule
      \bmname{lnm}             &           295 &    100\% \\
      \bmname{suffixtree}      &           718 &    100\% \\
      \bmname{kcfa}            &         2,031 &    100\% \\
      \bmname{snake}           &         6,559 &    100\% \\
      \rcell{\bmname{take5}}   & \rcell{6,558} & \rcell{0\%} \\
      \bmname{acquire}         &        19,532 &      5.45\% \\
      \bmname{tetris}          &        18,791 &    100\% \\
      \bmname{synth}           &        59,046 &    100\%
    \end{tabular}
  \end{tabular}
\end{table}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{data/strategy-overall-hopeful.pdf}
  \caption{Hopeful only, 88,992 scenarios. RandomB is average of 3 runs, max stddev <0.12}
  \label{f:strategy-hope}
\end{figure}


\subsection{RQ2 Head to Head}

\begin{itemize}
\item[$Q_{X/Y}$] Is strategy $X$ more successful than strategy $Y$ in this
  context?
\end{itemize}

\begin{figure}[t]
  \includegraphics[width=0.9\columnwidth]{data/head-to-head.pdf}
  \caption{Opt vs everyone, all scenarios}
  \label{f:head-to-head}
\end{figure}

\clearpage

