\bg{TODO rerun snake profile, have "snake" not "usnake" just in case}

%% ---


\subsection{tmp: definitions}

A benchmark is a family of programs.
There are $3^N$ program configurations (\emph{configurations} for short)
in each family, where $N$ is the number of \emph{migratable} modules.
These modules can depend on any number of additional \emph{contextual} modules.

A migration target (\emph{target}) is a set of modules that should be equipped
with types.
There are $2^N$ targets per benchmark.

FILL collected between July 2022 and today (ongoing).
quadT runtime between August 2022 and October 2022, since then, unable to get a huge
block of machines for the profile and boundary data.



\subsection{Data Collection}

du -h boundary
113M	uacquire
8.7M	ukcfa
17M	ulnm
26M	usnake
3.0M	usuffixtree
235M	usynth
27M	utake5
78M	utetris
348K	uzombie
984K	dungeon
328K	morsecode
40K	sieve
328K	fsm
328K	forth
1.2M	jpeg
16M	lnm
328K	mbta

du -h profile
631M	uacquire
75M	ukcfa
46M	ulnm
250M	snake
57M	usuffixtree
2.1G	usynth
89M	utake5
670M	utetris
3.9M	uzombie
18M	dungeon
1.9M	morsecode
220K	sieve
2.5M	fsm
4.9M	forth
11M	jpeg
46M	lnm
3.4M	mbta

ls -lh *out
-rw-r--r-- 1 ben users  30K Mar 31 16:23 00-morsecode.out
-rw-r--r-- 1 ben users  31K Sep 27  2022 01-forth.out
-rw-r--r-- 1 ben users  29K Sep 27  2022 02-fsm.out
-rw-r--r-- 1 ben users  32K Sep 27  2022 03-fsmoo.out
-rw-r--r-- 1 ben users  30K Sep 27  2022 04-mbta.out
-rw-r--r-- 1 ben users 3.5K Sep 27  2022 05-sieve.out
-rw-r--r-- 1 ben users 7.2M Sep 27  2022 06-acquire.out
-rw-r--r-- 1 ben users  91K Jan 24 12:28 07-dungeon.out
-rw-r--r-- 1 ben users  89K Sep 27  2022 08-jpeg.out
-rw-r--r-- 1 ben users 816K Sep 27  2022 09-kcfa.out
-rw-r--r-- 1 ben users 255K Sep 27  2022 10-lnm.out
-rw-r--r-- 1 ben users 2.4M Sep 27  2022 11-snake.out
-rw-r--r-- 1 ben users 283K Sep 27  2022 12-suffixtree.out
-rw-r--r-- 1 ben users 2.5M Sep 27  2022 13-take5.out
-rw-r--r-- 1 ben users 7.3M Jan 24 12:47 14-tetris.out
-rw-r--r-- 1 ben users  30K Sep 27  2022 15-zombie.out
-rw-r--r-- 1 ben users  23M Sep 27  2022 19-synth.out


FILL explain "u" setup, appendix?


% https://www.cloudlab.us/

Our experiments call for three kinds of data for every configuration across the benchmarks:
\begin{enumerate}
  \item Running time
  \item Boundary-profile output
  \item Statistical-profile output
\end{enumerate}

To collect one running time, we used nine runs of the configuration:
one throwaway run to warm up the Racket JIT and eight other runs to compute an average.
To collect boundary and statistical output, we ran each configuration once.
Thus we took $11 * \totalnumconfigs{} = \totalnummeasurements{}$ measurements in total.

We collected all the data on CloudLab~[CITE].
For running times, we used the Wisconsin cluster c220g1 
FILL: exception for morsecode.

FILL: exact racket/tr code available on zenodo

FILL: footnote, quadT in the works; report current status

boundary from another cluster, same racket
exceptions: dungeon, morsecode
profile, ditto

Tried two modes for statistical profiler: total time and self time.
Comparable, but total succeeds in a greater number of configurations.
There do exist configs where total fails and self succeeds.
Focus on total for most of paper (TODO where?)


\begin{table}[t]
  \caption{Datasets}
  \label{t:data-collection}

  \begin{tabular}{llll}
    Dataset           & Server & Racket & Typed Racket \\\midrule
    dungeon           & \machinename{c220g2} & \stdrkt{} &  \commitname{29ea3c10}{29ea3c105e0bd60b88c1fd195b54fa716863f690} \\
    morsecode         & \machinename{m510}   & same & \commitname{700506ca}{700506ca01393f211229101c47d8420f6d535de9} (cherry pick) \\
 %% quadT runtime  & \machinename{m510}   & same & same \\
    other runtime     & \machinename{c220g1} & same & default \\
    profilers         & \machinename{m510}   & same & same
  \end{tabular}

  \bigskip

  \begin{tabular}{llrrr}
    %% multi-cpu, multi-core machines .. but we didn't use that, right?
    Server & Site & CPU Speed & RAM & Disk \\\midrule
    \machinename{c220g1} & Wisconsin & 2.4GHz & 128GB & 480GB SSD \\
    \machinename{c220g2} & Wisconsin & 2.6GHz & 160GB & 480GB SSD \\
    \machinename{m510}   & Utah      & 2.0GHz &  64GB & 256GB SSD
  \end{tabular}
\end{table}




For our experiments, we consider every configuration as a starting point.
From the starting point, the goal is to reach a fast configuration without
removing types from a module.
(In other words, the \emph{target} is the set of typed modules in the start
configuration.)
A fast configuration runs at least as quickly as the untyped code.
Using the established terminology~\cite{vss-popl-2017,bbst-oopsla-2017},
we instatiate the Takikawa constant to 1x ($T=1$).

A very simple way to try improving performance is to toggle between
Deep and Shallow.
\citet{g-deep-shallow} provides justification.
Any configuration can reach 0, 1, or 2 other configurations by
toggling, that is, by changing all its typed modules to Deep or
changing all to Shallow.
(Only the untyped configuration can reach 0 others.)

First question: for how many configurations does $T=1$ present a debugging challenge?
More precisely, how many cannot reach $T=1$ by toggling?

(The untyped configuration can trivially reach $T=1$.
The fully-typed configuration can usually reach $T=1$, unless there are heavy
boundaries to untyped contextual modules.)

\Cref{t:baseline-trouble} shows that many benchmarks need help,
over 100k configurations in total.
The median \% that need help is 82\%.
Only \bmname{fsm} and \bmname{lnm} have low percentages: 8\% and 44\%.
FILL say more

\begin{table}[t]
  \caption{How many of the $3^N$ configurations have any overhead to begin with?}
  \label{t:baseline-trouble}
  \begin{tabular}[t]{l@{\qquad}l}
    \begin{tabular}[t]{lrr}
      Benchmark           & $3^N$ & \% Slow \\\midrule
      \bmname{sieve}      &     9 & 77.78\% \\
      \bmname{morsecode}  &    81 & 82.72\% \\
      \bmname{forth}      &    81 & 93.83\% \\
      \bmname{fsm}        &    81 & \ycell{76.54\%} \\
      \bmname{fsmoo}      &    81 & 83.95\% \\
      \bmname{mbta}       &    81 & 88.89\% \\
      \bmname{zombie}     &    81 & 91.36\% \\
      \bmname{dungeon}    &   243 & 99.59\% \\
      \bmname{jpeg}       &   243 & 94.65\% \\
    \end{tabular}
    \begin{tabular}[t]{lrr}
      Benchmark           & $3^N$ & \% Slow \\\midrule
      \bmname{lnm}        &   729 & \ycell{40.47\%} \\
      \bmname{suffixtree} &   729 & 98.49\% \\
      \bmname{kcfa}       &  2187 & 92.87\% \\
      \bmname{snake}      &  6561 & 99.97\% \\
      \bmname{take5}      &  6561 & 99.95\% \\
      \bmname{acquire}    & 19683 & 99.23\% \\
      \bmname{tetris}     & 19683 & 95.47\% \\
      \bmname{synth}      & 59049 & 99.99\%
    \end{tabular}
  \end{tabular}
\end{table}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{data/strategy-overall.pdf}
  \caption{Strategy head-to-head, mono only. Profiles on left. Agnostics on right.}
  \label{f:strategy-overall}
\end{figure}

\clearpage

