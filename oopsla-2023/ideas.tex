%% -----------------------------------------------------------------------------

When a developer confronts a newly created performance-debugging scenario, one
possible response is to erase the types and to return to the {\em status quo
ante\/}.  Another one is to use a profiling tool to reduce the overhead. If the
second alternative is chosen, the question arises 
\begin{quote} \em

 how the developer should modify the program to obtain a version with tolerable
 performance.
 
\end{quote}   
The type-migration lattice suggests a straightforward answer.  The developer
should either (i) add types to an untyped component or (ii) experiment with
toggling existing types from deep to shallow protection and vice versa.  After
all, the fully typed program with deep run-time checks is known to be at least
as performant as its untyped counterpart, and therefore migration is guaranteed
to eliminate any performance bottlenecks sooner or later. 

Deciding which of the alternatives to pursue in a given situation poses the
follow-up question 
\begin{quote} \em

how a developer should interpret the feedback from a profiling tool to
choose a modification.

\end{quote}   
Operationally, this question asks how the feedback is turned into a
modification at each step of the debugging process.

In contrast to the first question, the second one does not have a
straightforward answer.  At each step, the developer has to choose a specific
component to modify.  The sequence of choices should result in a migratory path
along which performance monotonically improves. Assuming that the chosen
profiling tool identifies components that contribute to the performance
degradation, it is rational for the programmer to rely on its feedback to narrow
down the set of candidate components.  Obtaining profiler feedback is just the
first step, however. Next the developer must use the feedback information to
rank the components and to identify the highest-priority one for adding type or
toggling its type protection. 

Stepping back, these two insights suggest an experiment to determine which
profiling tool combined with which strategy helps developers make progress with
performance debugging. For this experiment, the developer must choose a
profiling tool---statistical or feature-specific---and a modification strategy.
To determine the best combination(s) means to have developers work through a
large number of performance-debugging scenarios. The result should help guide a
developer with toggling or adding types to resolve performance-related
problems during type migration.

Since it is obviously impossible to ask a developer to work through thousands of
performance-debugging scenarios, an alternative experimental method is needed.
The rational-programmer method provides a framework for conducting such
large-scale systematic examinations.

The method is inspired by the well-established idea of rationality in
economics~\cite{mill1874essays, henrich2001search}.  In more detail, a (bounded)
rational agent is a mathematical model of an economic actor. Essentially, it
abstracts an actual economic actor to an entity that, in any given
transaction-scenario, acts strategically to maximize some kind of benefit.  Of
course, the strategy cannot be fully rational given the bounded nature of human
beings and the limits in available information. Hence, investigations in modern
times use rational agents that are bounded: they do not make maximally optimal
choices and settle for\emph{satisficing}~\cite{hs:satisfice} their goal.

Analogously, a rational programmer is a model of a developer who aims to resolve
a problem in a work context---a scenario----with bounded
resources. Specifically, it is an algorithm that implements a developer's
bounded strategy for \emph{satisficing} its goal. Using such algorithms, it is
possible to inspect the effectiveness of an approach on a large quantity of
scenarios. In turn, comparing the results of such runs for different rational
programmers can inform human programmers about the success chances of a strategy
in some specific work context. In short, a rational-programmer evaluation yields
insights into the pragmatic value of work strategies. So far, the rational
programmer has been used to evaluate strategies for debugging logical mistakes;
this paper presents the first application to a performance problem. 

\medskip

%% -----------------------------------------------------------------------------
\noindent\textbf{A Rational-Programmer Experiment for Performance Debugging.}
In the context of profiler-guided type migration, a rational programmer consists
of two interacting pieces.  The first is strategy-agnostic; it consumes a
program, measures its running time, and if the performance is tolerable,
stops. Otherwise it faces a performance-debugging scenario and asks for guidance
from the second, strategy-specific piece. This second piece profiles the given
program---using the feature-specific profiler or the statistical profiler---and
analyzes the profiling information. Based on this analysis, it modifies the
program by adding types to some untyped component or flipping the run-time
enforcement of types for a typed-untyped boundary. This modified version is
handed back to the first piece of the rational programmer.

In fact, as the discussion implies, there are numerous versions of the rational
programmer that differ in their strategy-specific piece. Each such version,
dubbed a \emph{mode}, can be applied to realistic performance-debugging
scenarios and use its specific strategy to resolve the bottleneck.  For any
given scenario, the mode either succeeds in eliminating the bottleneck or fails
because the strategy cannot propose a program modification or because the chosen
modification worsens the performance of the program. 

Data about successes and failures for different modes can shed light on the
usefulness of one profiling strategy compared to another. Intuitively, the
success rate of a mode (in)validates the hypothesis that its strategy has
pragmatic value. Similarly, the comparison of the success rates of two different
modes on the same scenario clarifies the relative pragmatic value of the
two. Collecting the data necessary for drawing reliable conclusions calls for an
experiment that pits a spread of strategies against each other on a large and
diverse set of performance-debugging scenarios. Of course, the experiment may
also reveal shortcomings of the profiling approach altogether---which then
demands additional research from tool creators. 
