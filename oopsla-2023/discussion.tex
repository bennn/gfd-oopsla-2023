The results of the rational-programmer experiment suggest a few concrete lessons
for the working programmer and also for language designers. Before diving into
the details, it is necessary to look at the data for some individual benchmarks
(section~\ref{subsec:data}). The data is illustrative of the general lessons
(section~\ref{subsec:lessons}). Finally, readers should be aware some specific
and some general threats to the validity of the data and the conclusions
(section~\ref{subsec:threats}).

%% -----------------------------------------------------------------------------
\begin{figure}[ht]
  \def\lbl#1{\bmname{#1}}
  \newcommand{\kkrow}[1]{\includegraphics[width=0.39\columnwidth]{data/sky/#1-feasible.pdf}}
    \begin{tabular}[t]{ll}
     \lbl{tetris}   & \lbl{synth} \\
     \kkrow{tetris} & \kkrow{synth} \\
    \end{tabular}
  \caption{Examples of migration lattices best navigates with optimistic strategies} \label{fig:success}
\end{figure}  


\subsection{Data from Individual Benchmarks} \label{subsec:data}

Figure~\label{f:strategy-overall} summarize the successes and failures across
all benchmarks. Some of the results for individual benchmarks match this profile
well. As figure~\ref{fig:success} shows, the \bmname{tetris} and \bmname{synth}
are examples of such benchmarks. The two benchmarks share a basic characteristic. 
They consist of numerous components with a complex dependency
graph. Additionally, both benchmarks suffer from a double-digit average
performance degradation~\cite{gtnffvf-jfp-2019}. 

\begin{figure}[ht]
  \def\lbl#1{\bmname{#1}}
  \newcommand{\kkrow}[1]{\includegraphics[width=0.39\columnwidth]{data/sky/#1-feasible.pdf}}
    \begin{tabular}[t]{ll}
     \lbl{morsecode} & \lbl{lnm} \\
     \kkrow{morsecode} & \kkrow{lnm} \\
    \end{tabular}
  \caption{Examples of migration lattices best navigated with random choices} \label{fig:random}
\end{figure}

For some of the benchmarks, the results look extremely different. The two most
egregious examples are shown in figure~\ref{fig:random}: \bmname{morsecode} and
\bmname{lmn}. In contrast to the above examples, these two are relatively small
and exhibit a rather low performance overhead of less than $2x$. 

\begin{figure}[ht]
  \def\lbl#1{\bmname{#1}}
  \newcommand{\kkrow}[1]{\includegraphics[width=0.39\columnwidth]{data/sky/#1-feasible.pdf}}
    \begin{tabular}[t]{ll}
     \lbl{mbta} & \lbl{take5} \\
     \kkrow{mbta} & \kkrow{take5} \\
    \end{tabular}
  \caption{Results for navigations in migration lattices with black holes} \label{fig:bh}
\end{figure}

Finally, some benchmarks exhibit pathological obstacles. Take a look at
figure~\ref{fig:bh}, which display empty data plots for \bmname{mbta} and
\bmname{take5}. Neither migration lattice of these benchmarks comes with any
hopeful performance-debugging scenarios. See section~\ref{sec:results}.  What
these two highlight is that the working programmer does not know the complete
migration lattice, and that general lessons must not assume so.

%% -----------------------------------------------------------------------------
\subsection{Lessons for All} \label{subsec:lessons}

Given the general results from the preceding section and the data from the
individual benchmarks (see preceding subsection and appendix), the experiment
suggests four lessons for working programmers and one for language designers. 

When a programmer faces a performance-debugging scenario, the question is
whether to reach for a profiling tool and what kind. The general results and the
results for many individual benchmarks give a clear answer. On one hand, the
statistical profiler is inferior to the boundary profiler for guiding a
programmer through the migration lattice. On the other hand, for some programs a
programmer is best off without any profiling tools.  Specifically, when a
programmer is confronted with an intolerable performance of a large mixed-typed
program, it is time to reach for the boundary profiler. By contrast, for a
program with a handful of components, a programmer should toggle all existing
types or add types to randomly chosen components until the performance is
tolerable.

When a programmer has reached for the boundary profiler, the question is how the
programmer should interpret its feedback. Again, the data implies a single
answer.  If the boundary profiler is able to identify a particular boundary as a
cause of the intolerable performance, the programmer is best served by
converting both sides of the boundary to use deep types. This modification
should prioritize toggling existing shallow types to deep before adding deep
types to untyped components. Prioritizing in this order follows from the data
for the optimistically cost-aware option, which is better is as good or better
than the data for other approaches. The second-best option is to pursue the
plainly optimistic approach; the data shows that it is equally good or even
better in some cases, but it requires more effort from the programmer. 

As a programmer pursues one of these two strategies, one of the resulting
programs may suffer from performance problems that are worse than the original
ones. In this case, the question is whether the programmer should continue with
the performance-debugging effort. Once again, the data clearly suggests that the
programmer is unlikely to find a configuration with tolerable performance. Since
the programmer is using one of the best strategies and the new
configuration is a performance-debugging scenario, the data tells us that no
other strategy can help.

Finally, a reader may wonder whether the programmer should relax the high
standards of eliminating the entire performance overhead.  That is, the question
is whether a mixed-typed program should run as fast as its (possibly
non-existent) untyped variant. But, the antenna data disagrees with relaxing the
standard. With the exceptions of low-overhead programs, a profiling strategy is
as likely to produce an overhead-free configuration as it is to produce a
configuration with some reasonably bounded ($3x$) overhead. 

Language designers can extract a single lesson from the data. Despite the
optimist of \citet{g-deep-shallow}, the addition of shallow type enforcement to
Typed Racket does not seem to help with the navigation of the migration
lattice. All conservative profiling strategies---which prioritize shallow over
deep---are inferior to all optimistic profiling stratgies---which prefer deep
enforcement. 

%% -----------------------------------------------------------------------------
\subsection{Threats to Validity} \label{subsec:threats}


Threats to validity:

internal
————
representative benchmarks: black holes, what happens in the wild 


profiler limitations, especially the boundary one — see class-based
programs

migration steps: (1) typing one module at a time 
                 (2) splitting large modules is perhaps better

strategies: missing strategies


external
—————
one language and its toolset 

real programmers may not have access to libraries etc

real programmers may not have the patience to act as rational programmers
but may have ``hunches'' that pay off in realistic scenarios 


