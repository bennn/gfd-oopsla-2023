The results of the rational-programmer experiment suggest a few concrete lessons
for the working programmer and also for language designers. Before diving into
the details, it is necessary to look at the data for some individual benchmarks
(section~\ref{subsec:data}). The data is illustrative of the general lessons
(section~\ref{subsec:lessons}). Finally, readers should be aware some specific
and some general threats to the validity of the data and the conclusions
(section~\ref{subsec:threats}).

%% -----------------------------------------------------------------------------
\begin{figure}[ht]
  \def\lbl#1{\bmname{#1}}
  \newcommand{\kkrow}[1]{\includegraphics[width=0.39\columnwidth]{data/sky/#1-feasible.pdf}}
    \begin{tabular}[t]{ll}
     \lbl{tetris}   & \lbl{synth} \\
     \kkrow{tetris} & \kkrow{synth} \\
    \end{tabular}
  \caption{Examples of migration lattices best navigates with optimistic strategies} \label{fig:success}
\end{figure}  

\subsection{Data from Individual Benchmarks} \label{subsec:data}

Figure~\label{f:strategy-overall} summarize the successes and failures across
all benchmarks. Some of the results for individual benchmarks match this profile
well. As figure~\ref{fig:success} shows, the \bmname{tetris} and \bmname{synth}
are examples of such benchmarks. The two benchmarks share a basic characteristic. 
They consist of numerous components with a complex dependency
graph. Additionally, both benchmarks suffer from a double-digit average
performance degradation~\cite{gtnffvf-jfp-2019}. 

\begin{figure}[ht]
  \def\lbl#1{\bmname{#1}}
  \newcommand{\kkrow}[1]{\includegraphics[width=0.39\columnwidth]{data/sky/#1-feasible.pdf}}
    \begin{tabular}[t]{ll}
     \lbl{morsecode} & \lbl{lnm} \\
     \kkrow{morsecode} & \kkrow{lnm} \\
    \end{tabular}
  \caption{Examples of migration lattices best navigated with random choices} \label{fig:random}
\end{figure}

For some of the benchmarks, the results look extremely different. The two most
egregious examples are shown in figure~\ref{fig:random}: \bmname{morsecode} and
\bmname{lmn}. In contrast to the above examples, these two are relatively small
and exhibit a rather low performance overhead of less than $2x$. 

\begin{figure}[ht]
  \def\lbl#1{\bmname{#1}}
  \newcommand{\kkrow}[1]{\includegraphics[width=0.39\columnwidth]{data/sky/#1-feasible.pdf}}
    \begin{tabular}[t]{ll}
     \lbl{mbta} & \lbl{take5} \\
     \kkrow{mbta} & \kkrow{take5} \\
    \end{tabular}
  \caption{Results for navigations in migration lattices with black holes} \label{fig:bh}
\end{figure}

Finally, some benchmarks exhibit pathological obstacles. Take a look at
figure~\ref{fig:bh}, which display empty data plots for \bmname{mbta} and
\bmname{take5}. Neither migration lattice of these benchmarks comes with any
hopeful performance-debugging scenarios. See section~\ref{sec:results}.  What
these two highlight is that the working programmer does not know the complete
migration lattice, and that general lessons must not assume so.

%% -----------------------------------------------------------------------------
\subsection{Lessons for All} \label{subsec:lessons}

Programmers should aim at approximating the performance of the completely
untyped configuration.


For a program with a handful of components, a programmer should randomly
add types or toggle all existing types until the performance is tolerable.
Other than in those cases, shallow type enforcement does not seem to
offer any advantages. 

In case, a programmer thinks that a large mixed-typed program exhibits
intolerable performance, it is time to reach for the boundary profiler.
Furthermore, when the boundary profiler is able to identify a particular
as a cause of the bad performance, the programmer is best served by
converting both sides of the boundary to use deep types.  This should be
done by toggling existing shallow types to deep, if possible; otherwise,
adding deep types to the untyped module.  In support of this argument, the
programmer can safely ignore assessing any characterestic of the program
or the migration step; the blindly optimistic approach is as good or
better than approaches that require additional effort. 

If a programmer uses the boundary profiler with an optimistic approach,
and the result is a performance degradation, then it is unlikely that any
of strategies presented here will help debug the performance problem. 


%% -----------------------------------------------------------------------------
\subsection{Threats to Validity} \label{subsec:threats}


Threats to validity:

internal
————
representative benchmarks: black holes, what happens in the wild 


profiler limitations, especially the boundary one — see class-based
programs

migration steps: (1) typing one module at a time 
                 (2) splitting large modules is perhaps better

strategies: missing strategies


external
—————
one language and its toolset 

real programmers may not have access to libraries etc

real programmers may not have the patience to act as rational programmers
but may have ``hunches'' that pay off in realistic scenarios 


