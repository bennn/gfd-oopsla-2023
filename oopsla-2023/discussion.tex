The results of the rational-programmer experiment suggest a few concrete lessons
for the developers and also for language designers. Before diving into
the details, it is necessary to look at the data for some individual benchmarks
(section~\ref{subsec:data}). The data is illustrative of the general lessons
(section~\ref{subsec:lessons}). Finally, readers should be aware some specific
and some general threats to the validity of the data and the conclusions
(section~\ref{subsec:threats}).

%% -----------------------------------------------------------------------------

\subsection{Data from Individual Benchmarks} \label{subsec:data}

\Cref{f:strategy-overall} summarize the successes and failures across
all benchmarks. Some of the results for individual benchmarks match this profile
well. As~\cref{fig:success} shows, the \bmname{tetris} and \bmname{synth}
are examples of such benchmarks. The two benchmarks share a basic characteristic. 
They consist of numerous components with a complex dependency
graph. Additionally, both benchmarks suffer from a double-digit average
performance degradation with deep types~\cite{gtnffvf-jfp-2019}.

\begin{figure}[h]
  \def\lbl#1{\bmname{#1}}
  \newcommand{\kkrow}[1]{\includegraphics[width=0.39\columnwidth]{data/sky/#1-feasible.pdf}}
    \begin{tabular}[t]{ll}
     \lbl{tetris}   & \lbl{synth} \\
     \kkrow{tetris} & \kkrow{synth} \\
    \end{tabular}
  \Description{Examples of migration lattices best navigated with optimistic strategies}
  \caption{Examples of migration lattices best navigated with optimistic strategies}
  \label{fig:success}
\end{figure}

For some of the benchmarks, the results look extremely different. The two most
egregious examples are shown in~\cref{fig:random}: \bmname{morsecode} and
\bmname{lmn}. In contrast to the above examples, these two benchmarks are relatively small
and exhibit a rather low worst-case overhead of less than $3$x~\cite{g-deep-shallow}.

\begin{figure}[h]
  \def\lbl#1{\bmname{#1}}
  \newcommand{\kkrow}[1]{\includegraphics[width=0.39\columnwidth]{data/sky/#1-feasible.pdf}}
    \begin{tabular}[t]{ll}
     \lbl{morsecode} & \lbl{lnm} \\
     \kkrow{morsecode} & \kkrow{lnm} \\
    \end{tabular}
  \Description{Examples of migration lattices best navigated with random choices}
  \caption{Examples of migration lattices best navigated with random choices}
  \label{fig:random}
\end{figure}

Finally, some benchmarks exhibit pathological obstacles. Take a look
at~\cref{fig:bh},
which display the empty plots for \bmname{mbta} and
\bmname{take5}. Neither migration lattice of these benchmarks comes with any
hopeful performance-debugging scenarios~(\cref{t:blackhole}).
Because a developer does not know the complete migration lattice
and therefore cannot predict whether a scenario is hopeful, general lessons
must not depend on the full lattice either.

\begin{figure}[ht]
  \def\lbl#1{\bmname{#1}}
  \newcommand{\kkrow}[1]{\includegraphics[width=0.39\columnwidth]{data/sky/#1-feasible.pdf}}
    \begin{tabular}[t]{ll}
     \lbl{mbta} & \lbl{take5} \\
     \kkrow{mbta} & \kkrow{take5} \\
    \end{tabular}
  \Description{Empty results for navigations in lattices with zero hopeful scenarios}
  \caption{Empty results for navigations in lattices with zero hopeful scenarios}
  \label{fig:bh}
\end{figure}

%% -----------------------------------------------------------------------------
\subsection{Lessons} \label{subsec:lessons}

Given the general results from the preceding section and the data from the
individual benchmarks~(see preceding subsection and
\ifappendix{\cref{s:bm-sky}}{supplementary material}),
the experiment
suggests three lessons for developers and one for language designers. 

When a developer faces a performance-debugging scenario, the question is
whether to reach for a profiling tool and what kind. The general results and the
results for many individual benchmarks give a clear, two-part answer.
First, the boundary profiler is superior to the statistical profiler for
navigating the migration lattice.
Second, this profiler works best on large mixed-typed programs.
For small programs with a handful of components and single-digit overheads,
the results show that toggling all existing types or randomly choosing a
boundary are more effective strategies.

When a developer has reached for the boundary profiler, the next question is how
to interpret its feedback. The data implies a single
answer.  If the boundary profiler is able to identify a particular boundary as a
cause of the intolerable performance, the developer is best served by
converting both sides of the boundary to use deep types. This modification
may prioritize toggling existing shallow types to deep before adding deep
types to untyped components. Prioritizing in this order follows from the data
for the cost-aware optimistic strategy which is on par with the (cost-unaware)
\emph{optimistic} strategy.

Finally, as a developer applies an optimistic strategy, configurations
along the migration path may suffer from performance problems that are
worse than the original ones. In this case, the question is whether the
developer should continue with the performance-debugging effort. The data
suggests that one setback is a bad sign (\pct{10} of configurations
succeed despite one setback) and anything more than two setbacks means
that success is highly unlikely.  Changing to a different strategy is
unlikely to help.

%%bg: unclear from the data ... we know N-loose 1x > strict 3x, but what about strict for each? also 1x might require longer paths
%% cd: rewrote to address Ben's point
Finally, a reader may wonder whether developers should relax the high
standards of eliminating the entire performance overhead.  That is, the
question is whether a mixed-typed program should run as fast as its
(possibly non-existent) untyped variant. But, the antenna data disagrees
with relaxing the standard. With the exceptions of low-overhead programs,
if a developer is willing to tolerate a small number of performance
degradations along the way, a profiling strategy is as likely to produce
a migration path that finds an overhead-free configuration as it is to
produce a configuration with some reasonably bounded ($3$x) overhead.

Language designers can extract a single lesson from the data.  The
addition of shallow types to the implementation of Typed
Racket~\cite{g-deep-shallow} does not seem to help with the navigation of
the migration lattice. All conservative profiling strategies, which
prioritize shallow over deep, are inferior to the optimistic profiling
strategies---which prefer deep enforcement. Possibly this is because
existing profiling technology is not sufficiently sensitive to detect the
aggregate cost of shallow's assertions.  Hence, language designers that
want to apply lessons from Typed Racket to another linguistic environment
are better off investing on deep types and a boundary profiler.  Further
research on techniques for compiling and profiling
gradually typed languages seems necessary to unlock the potential, if any,
of shallow types (see \Cref{sec:conclusion}).

%% -----------------------------------------------------------------------------
\subsection{Threats to Validity} \label{subsec:threats}

The validity of the conclusions may suffer from two kinds of threats to their
general validity.  The first one concerns the experimental setup. The second
category is about extrinsic aspects of the rational programmer method.

As for method-internal threats, the most important one is that the GTP Benchmarks
may not be truly representative of Racket programs in the wild. Several benchmarks
are somewhat small with simple dependency graphs and low performance overheads.
Since developers typically confront performance-debugging scenarios with large,
high-overhead programs, they will have to apply the general lessons with some
caution. The problem is that such programs may come with large hopeless regions in
the migration lattice. Concretely, once a program belongs to the part of the
lattice with high performance degradation, no profiling strategy will help the
developer escape it.
As~\cref{fig:bh2} illustrates for the \bmname{acquire} benchmark, the random
strategy works better for such large programs than any profiling strategy.  It
remains an open question how often hopeless scenarios occur in the wild.

\begin{figure}[ht]
  \def\lbl#1{\bmname{#1}}
  \newcommand{\kkrow}[1]{\includegraphics[width=0.39\columnwidth]{data/sky/#1-feasible.pdf}}
    \begin{tabular}[t]{ll}
     \lbl{acquire}   \\
     \kkrow{acquire} \\
    \end{tabular}
  \caption{An example of a large program with a large hopeless region}
  \label{fig:bh2}
  \Description{An example of a large program with a large hopeless region} 
\end{figure}  

The second most important internal threat concerns the design of the strategies.
While the set of strategies looks large and seems to cover a wide range of
approaches, it is far from complete. For example, certain combinations of the
chosen strategies---say, the optimistically cost-aware one with the random
one---might deliver better results than pursuing a pure strategy.  Another
weakness of the strategies is that their migration steps apply to single,
complete modules. One alternative is to migrate more than one module at a time. A
second alternative is to split modules into several typed and untyped
submodules~\cite{f:submodules}.  

A third internal thread is that the large scale of the experiment imposes
feasibility constraints on the collected data. Specifically, the experiment 
collects (and averages) only 8 performance measurements per scenario plus one 
for each profiler.

The design of the experiment attempts to mitigate all of the
method-internal threats. Specifically, the experiment setup pays tribute
to these threats with the use of the most standard suite of usable
benchmarks and the careful design of the strategies. Moreover, the
collected data was produced on dedicated machines, and was 
checked for instabilities or evidence of situations that may 
affect the measurements, such as warmup effects.  
Still, the reader must keep them in mind when drawing conclusions.

As for the method-external threats, the most important one is that the experiment
relies on a single language and its eco-system. While this choice is necessary for
an apples-to-apples comparison of strategies, it is unclear how the results apply
to other language and tool settings. Another aspect of this threat is that the
experiment involves only two profilers. While the statistical one is like those
found in most language eco-systems, the boundary profiler is unique to Racket. It
is possible that other language eco-systems come with profiling tools that might
just perform better than those two for some performance-debugging scenarios.

Stepping back, a reader may also question the entire rational-programmer idea as
an overly simplistic approximation of performance-debugging work in the real
world. But, programming language researchers know quite well that simplified
models have an illuminating power.  Similarly, empirical PL research has also
relied on highly simplified mental models of program execution for a long
time. As \citet{mdhs:wrong-data} report, ignorance of these simplifications can
produce wrong data---and did so for decades. Despite this problem, the simplistic
model acted as a compass that helped compiler writers improve their product
substantially over the same time period.

Like such models, the rational programmer is a simplified one.  While the
rational programmer experiment assumes that a developer takes all
information into account and sticks to a well-defined, possibly costly
process, a developer may make guesses, follow hunches, and take
shortcuts. Hence, the conclusions from the rational programmer
investigation may not match the experience of developers. Further
research that goes beyond the scope of this paper is necessary to
establish a connection between the behavior of rational programmers
and human developers.  

That said, the behavioral simplifications of the rational programmer are
analogous to the strategic simplifications that theoretical and practical models
make, and like those, they are necessary to make the rational programmer
experiment feasible. Despite all simplifications, section~\ref{sec:results}
demonstrates that the rational programmer method produces results that offer a
valuable lens for the community to understand some pragmatic aspects of
performance debugging of mixed-typed programs, and it does so at scale and in a
quantifiable manner.
