The results of the rational-programmer experiment suggest a few concrete lessons
for the developers and also for language designers. Before diving into
the details, it is necessary to look at the data for some individual benchmarks
(section~\ref{subsec:data}). The data is illustrative of general lessons
(section~\ref{subsec:lessons}).
A closer look at the scenarios yields additional lessons for language
designers~(\cref{s:specific-lessons}).
Finally, readers should be aware some specific
and some general threats to the validity of the data and the conclusions
(section~\ref{subsec:threats}).

%% -----------------------------------------------------------------------------

\subsection{Data from Individual Benchmarks} \label{subsec:data}

\Cref{f:strategy-overall} summarize the successes and failures across
all benchmarks. Some of the results for individual benchmarks match this profile
well. As~\cref{fig:success} shows, the \bmname{tetris} and \bmname{synth}
are examples of such benchmarks. The two benchmarks share a basic characteristic. 
They consist of numerous components with a complex dependency
graph. Additionally, both benchmarks suffer from a double-digit average
performance degradation with deep types~\cite{gtnffvf-jfp-2019}.

\begin{figure}[h]
  \def\lbl#1{\bmname{#1}}
  \newcommand{\kkrow}[1]{\includegraphics[width=0.39\columnwidth]{data/sky/#1-feasible.pdf}}
    \begin{tabular}[t]{ll}
     \lbl{tetris}   & \lbl{synth} \\
     \kkrow{tetris} & \kkrow{synth} \\
    \end{tabular}
  \Description{Examples of migration lattices best navigated with optimistic strategies}
  \caption{Examples of migration lattices best navigated with optimistic strategies}
  \label{fig:success}
\end{figure}

For some of the benchmarks, the results look extremely different. The two most
egregious examples are shown in~\cref{fig:random}: \bmname{morsecode} and
\bmname{lmn}. In contrast to the above examples, these two benchmarks are relatively small
and exhibit a rather low worst-case overhead of less than $3$x~\cite{g-deep-shallow}.

\begin{figure}[h]
  \def\lbl#1{\bmname{#1}}
  \newcommand{\kkrow}[1]{\includegraphics[width=0.39\columnwidth]{data/sky/#1-feasible.pdf}}
    \begin{tabular}[t]{ll}
     \lbl{morsecode} & \lbl{lnm} \\
     \kkrow{morsecode} & \kkrow{lnm} \\
    \end{tabular}
  \Description{Examples of migration lattices best navigated with random choices}
  \caption{Examples of migration lattices best navigated with random choices}
  \label{fig:random}
\end{figure}

Finally, some benchmarks exhibit pathological obstacles. Take a look
at~\cref{fig:bh},
which display the empty plots for \bmname{mbta} and
\bmname{take5}. Neither migration lattice of these benchmarks comes with any
hopeful performance-debugging scenarios~(\cref{t:blackhole}).
Because a developer does not know the complete migration lattice
and therefore cannot predict whether a scenario is hopeful, general lessons
must not depend on the full lattice either.

\begin{figure}[ht]
  \def\lbl#1{\bmname{#1}}
  \newcommand{\kkrow}[1]{\includegraphics[width=0.39\columnwidth]{data/sky/#1-feasible.pdf}}
    \begin{tabular}[t]{ll}
     \lbl{mbta} & \lbl{take5} \\
     \kkrow{mbta} & \kkrow{take5} \\
    \end{tabular}
  \Description{Empty results for navigations in lattices with zero hopeful scenarios}
  \caption{Empty results for navigations in lattices with zero hopeful scenarios}
  \label{fig:bh}
\end{figure}

%% -----------------------------------------------------------------------------
\subsection{General Lessons} \label{subsec:lessons}

Given the general results from the preceding section and the data from the
individual benchmarks~(see preceding subsection and
\ifappendix{\cref{s:bm-sky}}{supplementary material}),
the experiment
suggests three lessons for developers and one for language designers. 

When a developer faces a performance-debugging scenario, the question is
whether to reach for a profiling tool and what kind. The general results and the
results for many individual benchmarks give a clear, two-part answer.
First, the boundary profiler is superior to the statistical profiler for
navigating the migration lattice.
Second, this profiler works best on large mixed-typed programs.
For small programs with a handful of components and single-digit overheads,
the results show that toggling all existing types or randomly choosing a
boundary are more effective strategies.

When a developer has reached for the boundary profiler, the next question is how
to interpret its feedback. The data implies a single
answer.  If the boundary profiler is able to identify a particular boundary as a
cause of the intolerable performance, the developer is best served by
converting both sides of the boundary to use deep types. This modification
may prioritize toggling existing shallow types to deep before adding deep
types to untyped components. Prioritizing in this order follows from the data
for the cost-aware optimistic strategy which is on par with the (cost-unaware)
\emph{optimistic} strategy.

When a developer applies an optimistic strategy, configurations
along the migration path may suffer from performance problems that are
worse than the original ones. In this case, the question is whether the
developer should continue with the performance-debugging effort. The data
suggests that one setback is a bad sign (\pct{10} of configurations
succeed despite one setback) and anything more than two setbacks means
that success is highly unlikely.  Changing to a different strategy is
unlikely to help.

%%bg: unclear from the data ... we know N-loose 1x > strict 3x, but what about strict for each? also 1x might require longer paths
%% cd: rewrote to address Ben's point
A reader may also wonder whether developers should relax the high
standards of eliminating the entire performance overhead.  That is, the
question is whether a mixed-typed program should run as fast as its
(possibly non-existent) untyped variant. But, the antenna data disagrees
with relaxing the standard. With the exceptions of low-overhead programs,
if a developer is willing to tolerate a small number of performance
degradations along the way, a profiling strategy is as likely to produce
a migration path that finds an overhead-free configuration as it is to
produce a configuration with some reasonably bounded ($3$x) overhead.

Language designers can extract a single lesson from the data.  The addition of
shallow types to the implementation of Typed Racket~\cite{g-deep-shallow} does
not seem to help with the navigation of the migration lattice. All conservative
profiling strategies---those that prioritize shallow over deep---yield inferior
results compared to optimistic strategies---which prefer deep
enforcement. Possibly this is due to the state of profiling technology; no
existing profiler may be sufficiently sensitive to detect the aggregate cost of
shallow's assertions and point to cost reduction.  For now then, language
designers are better off investing in deep types and a boundary profiler.


\subsection{Specific Lessons}
\label{s:specific-lessons}

Given that none of the rational programmer strategies
succeed on all hopeful scenarios, a first step toward
future work is to understand why they fail and whether
a modified strategy might succeed.
The scenario data provide insights on failures,
and the migration lattices show where the opportunities are.

With the boundary profiler, the most common reason that strategies get stuck in
that there are no {internal} boundaries in the output.  Either there are no
expensive deep type boundaries~(the costs may come from shallow types),
or the boundaries involve at least one component that lives
outside the benchmark in library code.  This no-internal issue affects 395,000 scenarios.
Roughly one fourth of the scenarios are hopeless at any rate (127K).  The rest
are hopeful scenarios, and for the vast majority of these
(264K) the rational programmer can make one step of progress using statistical profiler data.
Adding statistical data as a
fallback when no boundary data is available may increase the
success rate.
This mixed strategy can serve as a starting point for future research.

With statistical profiler data, a huge number of scenarios (745K) get stuck
because there are no actionable boundaries in the data.
Unfortunately, there are several possible explanations:
the boundaries might point to library code, the main costs might point toward
essential computations rather than gradual typing checks, or the strategy
might fail to upgrade a candidate boundary.

Turning to the migration lattices, \cref{t:where-fast} shows where the
acceptable ($T  = 1$)
configurations are.
For a benchmark with $N$ components, it presents a vector with $N+1$ cells
that correspond to the levels in the migration lattice.
The leftmost cell represents the untyped configuration,
the second-to-left cell represents all $N * 2$ configurations with exactly one
typed component, and so on until the rightmost cell, which
represents all $2^N$ fully-typed configurations,
Each cell reports the number of acceptable configurations at its level.
If this number is zero the cell is red (\rboxtiny{}), otherwise
the cell is green (\gboxtiny{}).
All but a few cells are green, which means that acceptable configurations
are spread throughout the lattices.
Four benchmarks are exceptional:
\bmname{dungeon} and \bmname{take5} are entirely hopeless, while
\bmname{snake} and \bmname{synth} have acceptable configurations only
at the endpoints.
In the remaining benchmarks, there are many acceptable points to
reach for in future work.

\begin{table}[t]
  \caption{Which levels of the migration lattice have any acceptable configurations?}
  \label{t:where-fast}
  \newcommand{\lblrhs}{\hbox{}~\#acceptable}
  \begin{tabular}[t]{l@{~}l}
    \begin{tabular}{l@{~~}l}
      Benchmark & \lblrhs \\\midrule
%%    \bmname{sieve} & \begin{tabular}{lrrr} & \gboxlo{1} & \rboxlo{0} & \gboxlo{1} \\ \end{tabular} \\
    \bmname{morsecode} &
  \gboxlo{1}~ \gboxlo{2}~ \gboxlo{4}~ \gboxlo{4}~ \gboxlo{3}
    \\
    \bmname{forth} &
 \gboxlo{1}~ \gboxlo{2}~ \gboxlo{1}~\gboxlo{1}~\rboxlo{0}
    \\
    \bmname{fsm} &
   \gboxlo{1} ~ \gboxlo{3} ~ \gboxlo{4} ~ \gboxlo{7} ~ \gboxlo{4}
    \\
    \bmname{fsmoo} &
   \gboxlo{1} ~ \gboxlo{2} ~ \gboxlo{4} ~ \gboxlo{2} ~ \gboxlo{4}
    \\
    \bmname{mbta} &
   \gboxlo{1} ~ \gboxlo{4} ~ \gboxlo{4} ~ \rboxlo{0} ~ \rboxlo{0}
    \\
    \bmname{zombie} &
   \gboxlo{1} ~ \gboxlo{2} ~ \gboxlo{3} ~ \gboxlo{1} ~ \rboxlo{0}
    \\
    \bmname{dungeon} &
   \gboxlo{1} ~ \rboxlo{0} ~ \rboxlo{0} ~ \rboxlo{0} ~ \rboxlo{0} ~ \rboxlo{0}
    \\
    \bmname{jpeg} &
   \gboxlo{1} ~ \gboxlo{2} ~ \gboxlo{1} ~ \gboxlo{1} ~ \gboxlo{4} ~ \gboxlo{4}
  \end{tabular}
    &

    \begin{tabular}{l@{~~}l}
      Benchmark & \lblrhs{} by lattice level \\\midrule
    \bmname{lnm} &
  \gboxhi{1} ~ \gboxhi{9} ~ \gboxhi{38} ~ \gboxhi{93} ~ \gboxhi{138} ~ \gboxhi{116} ~ \gboxhi{39}
    \\
    \bmname{suffixtree} &
  \gboxhi{1} ~ \gboxhi{1} ~ \rboxhi{0} ~ \rboxhi{0} ~ \gboxhi{1} ~ \gboxhi{4} ~ \gboxhi{4}
    \\
    \bmname{kcfa} &
  \gboxhi{1} ~ \gboxhi{8} ~ \gboxhi{22} ~ \gboxhi{33} ~ \gboxhi{24} ~ \gboxhi{24} ~ \gboxhi{29} ~ \gboxhi{15}
    \\
    \bmname{snake} &
  \gboxhi{1} ~ \rboxhi{0} ~ \rboxhi{0} ~ \rboxhi{0} ~ \rboxhi{0} ~ \rboxhi{0} ~ \rboxhi{0} ~ \rboxhi{0} ~ \gboxhi{1}
    \\
    \bmname{take5} &
  \gboxhi{1} ~ \gboxhi{2} ~ \rboxhi{0} ~ \rboxhi{0} ~ \rboxhi{0} ~ \rboxhi{0} ~ \rboxhi{0} ~ \rboxhi{0} ~ \rboxhi{0}
    \\
    \bmname{acquire} &
  \gboxhi{1} ~ \gboxhi{8} ~ \gboxhi{28} ~ \gboxhi{51} ~ \gboxhi{45} ~ \gboxhi{16} ~ \gboxhi{2} ~ \rboxhi{0} ~ \rboxhi{0} ~ \rboxhi{0}
    \\
    \bmname{tetris} &
  \gboxhi{1} ~ \gboxhi{12} ~ \gboxhi{56} ~ \gboxhi{121} ~ \gboxhi{169} ~ \gboxhi{128} ~ \gboxhi{118} ~ \gboxhi{133} ~ \gboxhi{112} ~ \gboxhi{42}
    \\
    \bmname{synth} &
  \gboxhi{1} ~ \gboxhi{1} ~ \rboxhi{0} ~ \rboxhi{0} ~ \rboxhi{0} ~ \rboxhi{0} ~ \rboxhi{0} ~ \rboxhi{0} ~ \rboxhi{0} ~ \rboxhi{0} ~ \gboxhi{1}
    \\
  \end{tabular}
  \end{tabular}
\end{table}



%% -----------------------------------------------------------------------------
\subsection{Threats to Validity} \label{subsec:threats}

The validity of the conclusions may suffer from two kinds of threats to their
general validity.  The first one concerns the experimental setup. The second
category is about extrinsic aspects of the rational programmer method.

As for method-internal threats, the first and most important one is that the GTP Benchmarks
may not be truly representative of Racket programs in the wild. Several benchmarks
are somewhat small with simple dependency graphs and low performance overheads.
Since developers typically confront performance-debugging scenarios with large,
high-overhead programs, they will have to apply the general lessons with some
caution. The problem is that such programs may come with large hopeless regions in
the migration lattice. Concretely, once a program belongs to the part of the
lattice with high performance degradation, no profiling strategy will help the
developer escape it.
As~\cref{fig:bh2} illustrates for the \bmname{acquire} benchmark, the random
strategy works better for such large programs than any profiling strategy.  It
remains an open question how often hopeless regions occur in the wild.

\begin{figure}[ht]
  \def\lbl#1{\bmname{#1}}
  \newcommand{\kkrow}[1]{\includegraphics[width=0.39\columnwidth]{data/sky/#1-feasible.pdf}}
    \begin{tabular}[t]{ll}
     \lbl{acquire}   \\
     \kkrow{acquire} \\
    \end{tabular}
  \caption{An example of a large program with a large hopeless region}
  \label{fig:bh2}
  \Description{An example of a large program with a large hopeless region} 
\end{figure}

The second most important internal threat concerns the design of the strategies.
While the set of strategies covers the basic approaches to navigation,
it is far from complete. For example, certain combinations of the
chosen strategies---say, the optimistically cost-aware one with the random
one---might deliver better results than pursuing a pure strategy.  Another
weakness of the strategies is that their migration steps are small.
One alternative is to migrate a few modules at a time, similar to the toggling
strategy.
A second alternative is to split modules into several typed and untyped
submodules~\cite{f:submodules}.
On a more technical level, the rational programmers organize statistical
profile output by application (see \code{Idx} in~\cref{f:fsm-code:statistical})
rather than by module.
Grouping by module may lead to better recommendations.
%% TODO if by-module, how to group total time??

A third threat is that the rational programmers reject some configurations that
a human developer might accept.
If the average overhead of a configuration is within one standard deviation
of 1x overhead, the rational programmer accepts it.
A handful of configurations lie just outside this cutoff yet within the realm
of machine noise~\cite{mdhs:wrong-data}; for example,
\pct{3.5} of all configurations are rejected but have an absolute slowdown
of at most 100 milliseconds.
Accepting these borderline configurations could reduce the number of hopeless
scenarios in, say, \bmname{acquire}.
However, the 3x ``antennas''~(see \ifappendix{\cref{s:bm-sky}}{supplementary material}) include these borderline configurations and
nevertheless support our overall
conclusions.

Fourth, the large scale of the experiment imposes
feasibility constraints on the collected data. Specifically, the experiment 
collects (and averages) only eight performance measurements per scenario
and only one for each profiler.

The design of the experiment attempts to mitigate the
method-internal threats. For example, we collected data on
single-user machines and confirmed that \pct{99} of the running
times are stable~(\cref{sec:data}).
Still, the reader must keep these threats in mind when drawing conclusions.

As for the method-external threats, the most important one is that the experiment
relies on a single language and its eco-system. While this choice is necessary for
an apples-to-apples comparison of strategies, it is unclear how the results apply
to other language and tool settings. Another aspect of this threat is that the
experiment involves only two profilers. While the statistical one is like those
found in most language eco-systems, the boundary profiler is unique to Racket. It
is possible that other language eco-systems come with profiling tools that might
just perform better than those two for some performance-debugging scenarios.

Stepping back, a reader may also question the entire rational-programmer idea as
an overly simplistic approximation of performance-debugging work in the real
world. But, programming language researchers know quite well that simplified
models have an illuminating power.  Similarly, empirical PL research has also
relied on highly simplified mental models of program execution for a long
time. As \citet{mdhs:wrong-data} report, ignorance of these simplifications can
produce wrong data---and did so for decades. Despite this problem, the simplistic
model acted as a compass that helped compiler writers improve their product
substantially over the same time period.

Like such models, the rational programmer is a simplified one.  While the
rational programmer experiment assumes that a developer takes all
information into account and sticks to a well-defined, possibly costly
process, a developer may make guesses, follow hunches, and take
shortcuts. Hence, the conclusions from the rational programmer
investigation may not match the experience of developers. Further
research that goes beyond the scope of this paper is necessary to
establish a connection between the behavior of rational programmers
and human developers.  

That said, the behavioral simplifications of the rational programmer are
analogous to the strategic simplifications that theoretical and practical models
make, and like those, they are necessary to make the rational programmer
experiment feasible. Despite all simplifications, section~\ref{sec:results}
demonstrates that the rational programmer method produces results that offer a
valuable lens for the community to understand some pragmatic aspects of
performance debugging of mixed-typed programs, and it does so at scale and in a
quantifiable manner.
